plot(result, type = 3, pages = 1, ncol = 2)+
plot(result, what = "predicted", type = 4, pages = 0)
file.plot <- paste0("ssm_plots/", result$id, ".png")
ggsave(file = file.plot)
}
}
View(ssm_output)
View(ssm_output)
which(good_fits$id=="NA_201299_3")
plot(good_fits[[40]], "p", type=2, alpha=0.1)
plot(good_fits[[40]], what='fitted')
plot(ssm_list[[40]], "p", type=2, alpha=0.1)
plot(ssm_list[[40]], what='fitted')
res.rw <- osar(ssm_list[[40]])
plot(res.rw, type = "ts") | plot(res.rw, type = "qq")+
plot(res.rw, type = "acf") | plot_spacer()
## fit SSM to tricky turtles
# ran manually to be able to increase time step or try crw model
results <- list() # empty list
thisturt<-ssm_list[[40]] # enter index of turtle id
thisName <- unique(thisturt$id)#taking name from the dataframe
cat("doing", thisName, "\n")
histurt<-ssm_list[[40]] # enter index of turtle id
thisName <- unique(thisturt$id)#taking name from the dataframe
cat("doing", thisName, "\n")
result <- aniMotum::fit_ssm(thisturt, vmax = 3, model = "mp", time.step = 24, map = list(rho_o = factor(NA))) # can try with 24h or run crw instead of mp for poor fits
thisturt<-ssm_list[[40]] # enter index of turtle id
thisName <- unique(thisturt$id)#taking name from the dataframe
cat("doing", thisName, "\n")
result <- aniMotum::fit_ssm(thisturt, vmax = 3, model = "mp", time.step = 24, map = list(rho_o = factor(NA))) # can try with 24h or run crw instead of mp for poor fits
thisturt<-d[d$id=="NA_201299_3"]# enter index of turtle id
thisturt<-d[d$id=="NA_201299_3",]# enter index of turtle id
thisName <- unique(thisturt$id)#taking name from the dataframe
cat("doing", thisName, "\n")
result <- aniMotum::fit_ssm(thisturt, vmax = 3, model = "mp", time.step = 24, map = list(rho_o = factor(NA))) # can try with 24h or run crw instead of mp for poor fits
best_result <- result
# # visualise track
plot(best_result, "p", type=2, alpha=0.1)
result <- aniMotum::fit_ssm(thisturt, vmax = 3, model = "crw", time.step = 12, map = list(rho_o = factor(NA))) # can try with 24h or run crw instead of mp for poor fits
best_result <- result
# # visualise track
plot(best_result, "p", type=2, alpha=0.1)
result <- aniMotum::fit_ssm(thisturt, vmax = 3, model = "crw", time.step = 6, map = list(rho_o = factor(NA))) # can try with 24h or run crw instead of mp for poor fits
result <- aniMotum::fit_ssm(thisturt, vmax = 3, model = "mp", time.step = 6, map = list(rho_o = factor(NA))) # can try with 24h or run crw instead of mp for poor fits
result <- aniMotum::fit_ssm(thisturt, vmax = 3, model = "crw", time.step = 6, map = list(rho_o = factor(NA))) # can try with 24h or run crw instead of mp for poor fits
best_result <- result
# # visualise track
plot(best_result, "p", type=2, alpha=0.1)
result <- aniMotum::fit_ssm(thisturt, vmax = 3, model = "mp", time.step = 48, map = list(rho_o = factor(NA))) # can try with 24h or run crw instead of mp for poor fits
best_result <- result
# # visualise track
plot(best_result, "p", type=2, alpha=0.1)
which(good_fits$id=="QA91767_194461_0")
plot(ssm_list[[157]], "p", type=2, alpha=0.1)
plot(ssm_list[[157]], what='fitted')
res.rw <- osar(ssm_list[[157]])
plot(res.rw, type = "ts") | plot(res.rw, type = "qq")+
plot(res.rw, type = "acf") | plot_spacer()
thisturt<-d[d$id=="QA91767_194461_0",]# enter index of turtle id
thisName <- unique(thisturt$id)#taking name from the dataframe
cat("doing", thisName, "\n")
result <- aniMotum::fit_ssm(thisturt, vmax = 3, model = "mp", time.step = 24, map = list(rho_o = factor(NA))) # can try with 24h or run crw instead of mp for poor fits
result <- aniMotum::fit_ssm(thisturt, vmax = 3, model = "mp", time.step = 6, map = list(rho_o = factor(NA))) # can try with 24h or run crw instead of mp for poor fits
result <- aniMotum::fit_ssm(thisturt, vmax = 3, model = "mp", time.step = 12, map = list(rho_o = factor(NA))) # can try with 24h or run crw instead of mp for poor fits
good_fits
View(good_fits)
?fit_ssm
# fit SSM
fit2 <- fit_ssm(d, vmax = 4, model = "mp", time.step = 12, map = list(rho_o = factor(NA)))
# keep the ones that converged
good_fits<-fit2%>%filter(converged==T & pdHess==T)
fitids<-good_fits$id
ssm_list <- split(good_fits, seq(nrow(good_fits)))
for (i in 1:length(ssm_list)) {
result <- ssm_list[[i]]
thisName <- unique(result$id)#taking name from the dataframe
cat("doing", thisName, "\n")
# Check if the result is valid
if (!result$converged=="FALSE") {
# Generate the persistence estimates timeseries plot
plot(result, type = 3, pages = 1, ncol = 2)+
plot(result, what = "predicted", type = 4, pages = 0)
file.plot <- paste0("ssm_plots/", result$id, ".png")
ggsave(file = file.plot)
}
}
bad.fits<-c("QA43123_149087_11", "QA36853_48862_7", "NA_201299_3", "NA_201299_0", "NA_201296_1", "NA_201296_0", "K93088_96778_1", "K93087_96777_8", "K93087_96777_6", "K93085_72448_1", "_131869_1", "_131868_1", "131868_2", "_126272_4", "T7159_133762_4", "QA91767_194461_0", "QA58210_149086_3", "QA15577_45731_0", "QA81291_45771_10", "T7159_133762_7", "_126273_7", "_126272_0")
which(good_fits$id%in%bad.fits)
bad.fit.index<-which(good_fits$id%in%bad.fits)
#remove
good_fits1<-good_fits[!bad.fit.index]
#remove
good_fits1<-good_fits[-bad.fit.index]
# keep the ones that converged
good_fits<-fit2%>%filter(converged==T & pdHess==T)
fitids<-good_fits$id
bad.fit.index<-which(good_fits$id%in%bad.fits)
#remove
good_fits1<-good_fits[-bad.fit.index]
#remove
good_fits1<-good_fits[-bad.fit.index,]
saveRDS(good_fits1, "20240704ssm.Rds")
good_fits1
source("PA_paper/LoadPackages_PA.R")
source("PA_paper/PseudoFunctions.R")
out.dir='output.Predicted2024/'
in.csv='tags.csv'
getwd()
setwd(../)
setwd(..)
setwd("../")
getwd
source("PA_paper/PseudoFunctions.R")
source("PA-paper/PseudoFunctions.R")
out.dir='output.Predicted2024/'
in.csv='tags.csv'
# read in tag data
results<-readRDS("20240704ssm.Rds")
results
tags= results
tags<- results
tags$date
head(tags)
# filtered_results <- list()
# for (i in 1:length(results)) {
#   result <- results[[i]]
#   if (!is.null(result)) {
#     filtered_results[[result$id]] <- result
#   }
# }
floc <- lapply(results, grab, what = "predicted")
??grab
library(aniMotum)
# filtered_results <- list()
# for (i in 1:length(results)) {
#   result <- results[[i]]
#   if (!is.null(result)) {
#     filtered_results[[result$id]] <- result
#   }
# }
floc <- lapply(results, grab, what = "predicted")
# filtered_results <- list()
# for (i in 1:length(results)) {
#   result <- results[[i]]
#   if (!is.null(result)) {
#     filtered_results[[result$id]] <- result
#   }
# }
floc <- grab(results, what = "predicted")
floc.df<-do.call(rbind.data.frame, floc)
floc
tags = floc.df
#tags$date <- paste(gsub(" ","",paste(tags$Month,tags$Day,tags$Year,sep="/"), fixed=TRUE), " 12:00:00 GMT")
tags$dTime = as.POSIXct(strptime(as.character(tags$date), "%Y-%m-%d %H:%M", tz="GMT"))
tags = floc
#tags$date <- paste(gsub(" ","",paste(tags$Month,tags$Day,tags$Year,sep="/"), fixed=TRUE), " 12:00:00 GMT")
tags$dTime = as.POSIXct(strptime(as.character(tags$date), "%Y-%m-%d %H:%M", tz="GMT"))
glimpse(tags)
source("PA-paper/LoadPackages_PA.R")
source("PA-paper/PseudoFunctions.R")
# clear sim.alltags if exists in workspace
if (exists('sim.alltags')) rm(sim.alltags)
if (exists('sim.allbufftags')) rm(sim.allbufftags)
if (exists('sim.allbacktags')) rm(sim.allbacktags)
tags$tag<-tags$id
ids<-unique(tags$tag)
tags$long<-tags$lon
for (tagid in unique(tags$tag)[1:length(unique(tags$tag))]){
#Simulate CRW
sim.alldata <- createCRW(tags, tagid, n.sim=100)
#Simulate background
sim.background.alldata <- createbackgroundabsence(tags, tagid, n.sim=100)
}  # end for (tagid in unique(tags$ptt)){
source("PA-paper/PseudoFunctions.R")
for (tagid in unique(tags$tag)[1:length(unique(tags$tag))]){
#Simulate CRW
sim.alldata <- createCRW(tags, tagid, n.sim=100)
#Simulate background
sim.background.alldata <- createbackgroundabsence(tags, tagid, n.sim=100)
}  # end for (tagid in unique(tags$ptt)){
for (tagid in unique(tags$tag)[1:length(unique(tags$tag))]){
#Simulate CRW
sim.alldata <- createCRW(tags, tagid, n.sim=100)
#Simulate background
sim.background.alldata <- createbackgroundabsence(tags, tagid, n.sim=100)
}  # end for (tagid in unique(tags$ptt)){
source("PA-paper/PseudoFunctionsHPC.R")
for (tagid in unique(tags$tag)[2:length(unique(tags$tag))]){
#Simulate CRW
sim.alldata <- createCRW(tags, tagid, n.sim=100)
#Simulate background
sim.background.alldata <- createbackgroundabsence(tags, tagid, n.sim=100)
}  # end for (tagid in unique(tags$ptt)){
pt
# read in tag data
results<-readRDS("20240704ssm.Rds")
results$id
# ## fit SSM to tricky turtles
failed<-which(!fit2$id%in%good_fits1$id)
# # how many points per day on average
filtered_point_data<-read.csv("20240704filteredFGPS&ARGOS.csv")
glimpse(filtered_point_data)
# # how many points per day on average
filtered_point_data<-read.csv("20240704filteredFGPS&ARGOS.csv")%>%mutate(DateTime=as.POSIXct(DateTime, format="%Y-%m-%d %H:%M:%S", tz="GMT"))
glimpse(filtered_point_data)
# # how many points per day on average
# filtered_point_data<-read.csv("20240704filteredFGPS&ARGOS.csv")%>%mutate(DateTime=as.POSIXct(DateTime, format="%Y-%m-%d %H:%M:%S", tz="GMT"))
# for estimating time steps for ssm
filtered_point_data%>%
group_by(id, as.Date(DateTime))%>%
dplyr::summarise(n=n())%>%group_by(id)%>%
summarise(mean_id=mean(n))%>%summarise(mean_all=mean(mean_id)) # 6 for filtered- could get points every 4hrs; 8 for unfiltered (every 3 hrs)
# combined<-read.csv("R-Sh-G-FGPSARGOS.raw.csv") #raw data
# trying with filtered data
combined<-filtered_point_data
# if previous timestamp is more than 3 days previous, assign a new id
combined_split<-combined%>%
group_by(id)%>%
arrange(DateTime)%>%
mutate(timediff=DateTime-lag(DateTime),
id_3daygaps=ifelse(is.na(timediff), 0, ifelse(timediff>259200, 1, 0)),
cumsum=cumsum(id_3daygaps),
id_split=paste(id, cumsum, sep="_"))
# note that timediff is na for the first interval of each id, which was causing the first id_3daygaps of each id1 group to be na - fixed now.
glimpse(combined_split)
table(combined_split$id_split)
# retain only tracks with at least 20 locations?
combined20plus<-combined_split%>%group_by(id_split)%>%filter(n()>=20) |> droplevels()
table(combined20plus$id_split)
## read data, set GPS (G) to Generic Locations (GL) so user-supplied uncertainty
##  estimates can be used.
GPS4ssm <- combined20plus |> ungroup() |>
#dplyr::select(-1) |>
#mutate(lc = ifelse(lc == "G", "GL", lc)) # turning this off will use AniMotums defaults for GPS
## set up table of SD's by # Sats - this assumes the errors are symmetric. You
##   could replace the point estimates with the upper CI but the distances
##   are not that different
## estimates are from Webster et al. 2022
GPSerr.df <- tibble(qi = 4:11,
x.sd = c(937.02,
630.75,
421.24,
384.67,
159.14,
13.19,
12.45,
10.67),
y.sd = x.sd)
## read data, set GPS (G) to Generic Locations (GL) so user-supplied uncertainty
##  estimates can be used.
GPS4ssm <- combined20plus |> ungroup() #|>
#dplyr::select(-1) |>
#mutate(lc = ifelse(lc == "G", "GL", lc)) # turning this off will use AniMotums defaults for GPS
## set up table of SD's by # Sats - this assumes the errors are symmetric. You
##   could replace the point estimates with the upper CI but the distances
##   are not that different
## estimates are from Webster et al. 2022
GPSerr.df <- tibble(qi = 4:11,
x.sd = c(937.02,
630.75,
421.24,
384.67,
159.14,
13.19,
12.45,
10.67),
y.sd = x.sd)
## merge SD's with location data
tmp <- left_join(GPS4ssm, GPSerr.df, by = "qi") |> dplyr::select(!c(id))
# try for first 5
ids<-unique(tmp$id_split)
## use aniMotum::format_data to simplify set up for fitting SSM
d <- format_data(tmp, id="id_split", date = "DateTime", coord = c("lon","lat"))
# fit SSM
fit <- fit_ssm(d, vmax = 4, model = "mp", time.step = 12, map = list(rho_o = factor(NA)))
glimpse(d)
## read data, set GPS (G) to Generic Locations (GL) so user-supplied uncertainty
##  estimates can be used.
GPS4ssm <- combined20plus |> ungroup() |>
dplyr::select(-1) #|>
#mutate(lc = ifelse(lc == "G", "GL", lc)) # turning this off will use AniMotums defaults for GPS
## merge SD's with location data
tmp <- left_join(GPS4ssm, GPSerr.df, by = "qi") |> dplyr::select(!c(id))
# try for first 5
ids<-unique(tmp$id_split)
## use aniMotum::format_data to simplify set up for fitting SSM
d <- format_data(tmp, id="id_split", date = "DateTime", coord = c("lon","lat"))
# fit SSM
fit <- fit_ssm(d, vmax = 4, model = "mp", time.step = 12, map = list(rho_o = factor(NA)))
# # how many points per day on average
filtered_point_data<-read.csv("20240704filteredFGPS&ARGOS.csv")%>%mutate(DateTime=as.POSIXct(DateTime, format="%Y-%m-%d %H:%M:%S", tz="GMT"))
# for estimating time steps for ssm
filtered_point_data%>%
group_by(id, as.Date(DateTime))%>%
dplyr::summarise(n=n())%>%group_by(id)%>%
summarise(mean_id=mean(n))%>%summarise(mean_all=mean(mean_id)) # 6 for filtered- could get points every 4hrs; 8 for unfiltered (every 3 hrs)
# combined<-read.csv("R-Sh-G-FGPSARGOS.raw.csv") #raw data
# trying with filtered data
combined<-filtered_point_data
# if previous timestamp is more than 3 days previous, assign a new id
combined_split<-combined%>%
group_by(id)%>%
arrange(DateTime)%>%
mutate(timediff=DateTime-lag(DateTime),
id_3daygaps=ifelse(is.na(timediff), 0, ifelse(timediff>259200, 1, 0)),
cumsum=cumsum(id_3daygaps),
id_split=paste(id, cumsum, sep="_"))
# note that timediff is na for the first interval of each id, which was causing the first id_3daygaps of each id1 group to be na - fixed now.
glimpse(combined_split)
table(combined_split$id_split)
# retain only tracks with at least 20 locations?
combined20plus<-combined_split%>%group_by(id_split)%>%filter(n()>=20) |> droplevels()
table(combined20plus$id_split)
## read data, set GPS (G) to Generic Locations (GL) so user-supplied uncertainty
##  estimates can be used.
GPS4ssm <- combined20plus |> ungroup() #|>
#dplyr::select(-1) |>
#mutate(lc = ifelse(lc == "G", "GL", lc)) # turning this off will use AniMotums defaults for GPS
## merge SD's with location data
tmp <- left_join(GPS4ssm, GPSerr.df, by = "qi") |> dplyr::select(!c(id))
# try for first 5
ids<-unique(tmp$id_split)
## use aniMotum::format_data to simplify set up for fitting SSM
d <- format_data(tmp, id="id_split", date = "DateTime", coord = c("lon","lat"))
# fit SSM
fit <- fit_ssm(d, vmax = 4, model = "mp", time.step = 12, map = list(rho_o = factor(NA)))
glimpse(d)
## read data, set GPS (G) to Generic Locations (GL) so user-supplied uncertainty
##  estimates can be used.
GPS4ssm <- combined20plus |> ungroup() |>
#dplyr::select(-1) |>
mutate(lc = ifelse(lc == "G", "GL", lc)) # turning this off will use AniMotums defaults for GPS
## merge SD's with location data
tmp <- left_join(GPS4ssm, GPSerr.df, by = "qi") |> dplyr::select(!c(id))
# try for first 5
ids<-unique(tmp$id_split)
## use aniMotum::format_data to simplify set up for fitting SSM
d <- format_data(tmp, id="id_split", date = "DateTime", coord = c("lon","lat"))
# fit SSM
fit <- fit_ssm(d, vmax = 4, model = "mp", time.step = 12, map = list(rho_o = factor(NA)))
## read data, set GPS (G) to Generic Locations (GL) so user-supplied uncertainty
##  estimates can be used.
GPS4ssm <- combined20plus |> ungroup() |>
dplyr::select(-1) |>
mutate(lc = ifelse(lc == "G", "GL", lc)) # turning this off will use AniMotums defaults for GPS
## merge SD's with location data
tmp <- left_join(GPS4ssm, GPSerr.df, by = "qi") |> dplyr::select(!c(id))
# try for first 5
ids<-unique(tmp$id_split)
## use aniMotum::format_data to simplify set up for fitting SSM
d <- format_data(tmp, id="id_split", date = "DateTime", coord = c("lon","lat"))
# fit SSM
fit <- fit_ssm(d, vmax = 4, model = "mp", time.step = 12, map = list(rho_o = factor(NA)))
table(combined20plus$id_split)
# fit SSM
fit <- fit_ssm(d, vmax = 4, model = "mp", time.step = 12)
## install aniMotum dev branch (do first time only)
# remotes::install_github("ianjonsen/aniMotum@dev")
require(aniMotum)
SB<-read.csv("SBArgos_foraging_only.csv")
# Brought csv in to QGIS and visualised individual tracks. Removed 3 possible breeding/courtship in QGIS. When bringing DateTime field into QGIS make sure it comes in as "Date and Time" format to not lose timestamps.
GL<-read.csv("GL_foraging_only.csv")
# # # Retain only foraging- pulled out in QGIS:
Raine<-read.csv(file="Raine_just_foraging.csv", header=T)
#####################
SB$Deploy.site<-"SB"
Raine$Deploy.site<-"RI"
GL$Deploy.site<-"GL"
# Raine post-breeding tracks already removed
# Shoalwater, removed first 24h
Sh.inds<-unique(SB$id)
SB$DateTime<-as.POSIXct(SB$DateTime, format='%Y/%m/%d %H:%M:%S')
SB <-SB%>%
arrange(id, DateTime) %>%
group_by(id) %>%
filter(!(DateTime < min(DateTime) + hours(24)))
# Gladstone
GL$DateTime<-as.POSIXct(GL$DateTime, format='%Y/%m/%d %H:%M:%S', tz="GMT")
GL <-GL%>%
arrange(id, DateTime) %>%
group_by(id) %>%
filter(!(DateTime < min(DateTime) + hours(24))) #remove first 24 hours
glimpse(GL)
Raine$DateTime<-as.POSIXct(Raine$DateTime, format='%Y/%m/%d %H:%M:%S', tz="GMT")
SB$lc<-as.character(SB$lc)
common_cols <- intersect(colnames(GL), colnames(SB))
GLRISB<-rbind(subset(GL, select=common_cols), subset(Raine, select=common_cols), subset(SB, select=common_cols))
GLRISB<-GLRISB[!(GLRISB$lon== "" | is.na(GLRISB$lon)), ] # remove empty rows
#GLRISB<-GLRISB%>%group_by(id, Type)%>%distinct(lat,lon,DateTime, .keep_all=T) # remove duplicates
#write.csv(GLRISB, "20240704rawFGPS&ARGOS.csv")
# ddfilter
GPS<-GLRISB%>%filter(Type=="FGPS")%>%ungroup()
ARGOS<-GLRISB%>%filter(Type=="ARGOS")%>%ungroup()
GPS<-as.data.frame(GPS)
ARGOS<-as.data.frame(ARGOS)
dd.GPS<-ddfilter(GPS, vmax=9.9, qi=4, ia=90, vmaxlp=2.0)
library(stars)
library(readr)
library(SDLfilter)
library(raster)
library(lubridate)
library(adehabitatLT)
library(momentuHMM)
library(rjags)
library(bsam)
require(tidyverse)
## install aniMotum dev branch (do first time only)
# remotes::install_github("ianjonsen/aniMotum@dev")
require(aniMotum)
library(move)
library(ctmm)
library(adehabitatLT)
require(patchwork)
# ddfilter
GPS<-GLRISB%>%filter(Type=="FGPS")%>%ungroup()
ARGOS<-GLRISB%>%filter(Type=="ARGOS")%>%ungroup()
GPS<-as.data.frame(GPS)
ARGOS<-as.data.frame(ARGOS)
dd.GPS<-ddfilter(GPS, vmax=9.9, qi=4, ia=90, vmaxlp=2.0)
dd.ARGOS<-ddfilter(ARGOS, vmax=9.9, qi=1, ia=90, vmaxlp=2.0)
# insufficient data to apply ddfilter_loop to;
# QA45701_96774, QA46115_88076
dd.data<-rbind(dd.GPS, dd.ARGOS)
dup.data<-dupfilter(dd.data)
library(raster)
library(sp)
# Load the raster data
raster_data <- raster("enviro datasets/100482_RelativeExtentsLayer/ITEM_REL_mosaic_1987_2015.tif")
# layer available at https://ecat.ga.gov.au/geonetwork/srv/eng/catalog.search#/metadata/100482
# Load the point data
LatLon <- data.frame(y=dup.data$lat, x=dup.data$lon)
coordinates(LatLon) <- ~x+y
mypoints = SpatialPoints(LatLon, proj4string = CRS("+init=epsg:4326"))
# Extract the raster values at the locations of the points
myvalues = raster::extract(raster_data, mypoints)
all<-cbind(dd.data, myvalues)
crs(raster_data)
all<-cbind(dup.data, myvalues)
# Select the points with values less than 9
filtered_point_data <- all |> filter(myvalues<9)
# there is still a spurious location?
write.csv(filtered_point_data, file="20240704filteredFGPS&ARGOS.csv")
# # how many points per day on average
#filtered_point_data<-read.csv("20240704filteredFGPS&ARGOS.csv")%>%mutate(DateTime=as.POSIXct(DateTime, format="%Y-%m-%d %H:%M:%S", tz="GMT"))
# for estimating time steps for ssm
filtered_point_data%>%
group_by(id, as.Date(DateTime))%>%
dplyr::summarise(n=n())%>%group_by(id)%>%
summarise(mean_id=mean(n))%>%summarise(mean_all=mean(mean_id)) # 6 for filtered- could get points every 4hrs; 8 for unfiltered (every 3 hrs)
# combined<-read.csv("R-Sh-G-FGPSARGOS.raw.csv") #raw data
# trying with filtered data
combined<-filtered_point_data
# if previous timestamp is more than 3 days previous, assign a new id
combined_split<-combined%>%
group_by(id)%>%
arrange(DateTime)%>%
mutate(timediff=DateTime-lag(DateTime),
id_3daygaps=ifelse(is.na(timediff), 0, ifelse(timediff>259200, 1, 0)),
cumsum=cumsum(id_3daygaps),
id_split=paste(id, cumsum, sep="_"))
# note that timediff is na for the first interval of each id, which was causing the first id_3daygaps of each id1 group to be na - fixed now.
glimpse(combined_split)
table(combined_split$id_split)
# retain only tracks with at least 20 locations?
combined20plus<-combined_split%>%group_by(id_split)%>%filter(n()>=20) |> droplevels()
table(combined20plus$id_split)
## merge SD's with location data
tmp <- left_join(GPS4ssm, GPSerr.df, by = "qi") |> dplyr::select(!c(id))
# try for first 5
ids<-unique(tmp$id_split)
## use aniMotum::format_data to simplify set up for fitting SSM
d <- format_data(tmp, id="id_split", date = "DateTime", coord = c("lon","lat"))
# fit SSM
fit <- fit_ssm(d, vmax = 4, model = "mp", time.step = 12, map = list(rho_o = factor(NA)))
# fit SSM
fit <- fit_ssm(d, vmax = 4, model = "mp", time.step = 12, map = list(rho_o = factor(NA)))
glimpse(d)
## read data, set GPS (G) to Generic Locations (GL) so user-supplied uncertainty
##  estimates can be used.
GPS4ssm <- combined20plus |> ungroup() #|>
glimpse(GPS4ssm)
## merge SD's with location data
tmp <- left_join(GPS4ssm, GPSerr.df, by = "qi") |> dplyr::select(!c(id))
# try for first 5
ids<-unique(tmp$id_split)
## use aniMotum::format_data to simplify set up for fitting SSM
d <- format_data(tmp, id="id_split", date = "DateTime", coord = c("lon","lat"))
glimpse(D)
glimpse(d)
# fit SSM
fit <- fit_ssm(d, vmax = 4, model = "mp", time.step = 12, map = list(rho_o = factor(NA)))
## install aniMotum dev branch (do first time only)
# remotes::install_github("ianjonsen/aniMotum@dev")
require(aniMotum)
# fit SSM
fit <- fit_ssm(d, vmax = 4, model = "mp", time.step = 12, map = list(rho_o = factor(NA)))
# fit SSM
fit <- aniMotum::fit_ssm(d, vmax = 4, model = "mp", time.step = 12, map = list(rho_o = factor(NA)))
d
## read data, set GPS (G) to Generic Locations (GL) so user-supplied uncertainty
##  estimates can be used.
GPS4ssm <- combined20plus |> ungroup() |>
# dplyr::select(-1) |>
mutate(lc = ifelse(lc == "G", "GL", lc)) # turning this off will use AniMotums defaults for GPS
## merge SD's with location data
tmp <- left_join(GPS4ssm, GPSerr.df, by = "qi") |> dplyr::select(!c(id))
# try for first 5
ids<-unique(tmp$id_split)
## use aniMotum::format_data to simplify set up for fitting SSM
d <- format_data(tmp, id="id_split", date = "DateTime", coord = c("lon","lat"))
# fit SSM
fit <- aniMotum::fit_ssm(d, vmax = 4, model = "mp", time.step = 12, map = list(rho_o = factor(NA)))
