---
title: "Untitled"
output: html_document
date: "2024-01-16"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
source("LoadPackages_PA.R")
library(sf)
#library(iml)
```


```{r}
AppendedAll<-read.csv("20240806AppendedAll.csv")
#remove spatial/temporal where there is too much missing predictor data (eReefs BGC goes until 2019 only)
AppendedAll%>%arrange(date)%>%slice_sample(n=50000)%>%visdat::vis_miss(warn_large_data = F)
filtered<-AppendedAll%>%filter(!is.na(Secchi))%>%filter(!is.na(date))

background<-filtered%>%
    dplyr::select(!c(X.2, X.1, X))%>%
  mutate(COMMUNITY=factor(COMMUNITY),
         builtfeature=factor(builtfeatures),
         geohab=factor(geohab),
         geomorph=factor(geomorph),
         dist2coast=dist2coast1_2)%>%
  filter(data_type!="crw")%>%
  mutate(data_type=ifelse(data_type=="track", 1, 0))

crw<-filtered%>%
    dplyr::select(!c(X.2, X.1, X))%>%
  mutate(COMMUNITY=factor(COMMUNITY),
         builtfeature=factor(builtfeatures),
         geohab=factor(geohab),
         geomorph=factor(geomorph),
         dist2coast=dist2coast1_2)%>%
  filter(data_type!="background")%>%
  mutate(data_type=ifelse(data_type=="track", 1, 0))
```

```{r}
# separate GL and unmodified turtles

metadata<-read.csv("tracked.turtls.csv")
# glimpse(metadata)


metadata$ID<-paste(metadata$Primary.tag, metadata$PTT, sep="_")
metadata<-metadata%>%select(c(ID, Deploy.site, Sex, Maturity, CCL))
crw$ID<-sub("_[^_]*$", "", crw$id)
background$ID<-sub("_[^_]*$", "", background$id)

crw<-crw%>%left_join(metadata, by="ID")
crw<-crw%>%mutate(Sex=factor(Sex), Maturity=factor(Maturity))

background<-background%>%left_join(metadata, by="ID")
background<-background%>%mutate(Sex=factor(Sex), Maturity=factor(Maturity))

```

```{r}
data_join_b<-background


GL_b<-data_join_b%>%filter(Deploy.site=="GL")
unmodified_b<-data_join_b%>%filter(!Deploy.site%in%c("GL"))

data_join<-crw

GL<-data_join%>%filter(Deploy.site=="GL")
unmodified<-data_join%>%filter(!Deploy.site%in%c("GL"))
```



```{r}
# #sample data (75% training 25% testing)
# # library(caret)
# set.seed(123)
# splitIndex <- createDataPartition(data_join$data_type, p = 0.75, list = FALSE)
# train_data <- data_join[splitIndex, ]
# test_data <- data_join[-splitIndex, ]
# 
# # GL sample
# splitIndexGL <- createDataPartition(GL$data_type, p = 0.75, list = FALSE)
# train_dataGL <- GL[splitIndexGL, ]
# test_dataGL <- GL[-splitIndexGL, ]
# # unmodified sample
# splitIndexunmodified <- createDataPartition(unmodified$data_type, p = 0.75, list = FALSE)
# train_dataunmodified <- unmodified[splitIndexunmodified, ]
# test_dataunmodified <- unmodified[-splitIndexunmodified, ]
```
```{r}
# set.seed(123)
# splitIndex_b <- createDataPartition(data_join_b$data_type, p = 0.75, list = FALSE)
# train_data_b <- data_join_b[splitIndex_b, ]
# test_data_b <- data_join_b[-splitIndex_b, ]
# 
# # GL sample
# splitIndexGL_b <- createDataPartition(GL_b$data_type, p = 0.75, list = FALSE)
# train_dataGL_b <- GL_b[splitIndexGL_b, ]
# test_dataGL_b <- GL_b[-splitIndexGL_b, ]
# # unmodified sample
# splitIndexunmodified_b <- createDataPartition(unmodified_b$data_type, p = 0.75, list = FALSE)
# train_dataunmodified_b <- unmodified_b[splitIndexunmodified_b, ]
# test_dataunmodified_b <- unmodified_b[-splitIndexunmodified_b, ]
```

correlation matrix
```{r}
data.num<-select_if(data_join, is.numeric)# include only numeric
data4cor<-data.num[,c(3:14, 16, 19:21, 24:29)]
corr<-cor(data4cor, method='pearson', use='pairwise.complete.obs')
round(corr, 3)
# extract pairs with 0.7 coeficient or higher
w <- which(abs(corr)>0.8 & row(corr)<col(corr), arr.ind=TRUE)
## reconstruct names from positions
high_cor <- matrix(colnames(corr)[w],ncol=2)
high_cor
```
Remove highly (>0.7) correlated variables from subsequent analysis
1) bathy, dist2coast, dist2rivers, dist2recboats, 
2) mmpexposure vs dist2coast and bathy
3) mmpfrequency vs dist2coast and bathy
4) mmpexposure vs mmpfrequency
5) ruggedness vs slope
6) dea mangrovs vs bathy & dist2coast
7) Secchi vs bathy, mmpfrequency and mmpexposure

Keep: mmpexposure1, bathy, dist2coast, ruggedness
-> run 1 model with bathy and 1 with dist2coast?
Remove: Secchi, deamangroves, slope, dist2rivers, dist2boatramps


With 0.8 as the cutoff for correlation, 
remove: dist2boats, slope, deamangrove and mmpfrequency

```{r}
data.num<-select_if(data_join_b, is.numeric)# include only numeric
data4cor<-data.num[,c(5:7,9:29)]
corr<-cor(data4cor, method='pearson', use='pairwise.complete.obs')
round(corr, 3)
# extract pairs with 0.7 coeficient or higher
w <- which(abs(corr)>=0.7 & row(corr)<col(corr), arr.ind=TRUE)
## reconstruct names from positions
high_cor <- matrix(colnames(corr)[w],ncol=2)
high_cor
```
With background data, dist2recboat, dist2rivers, dist2coast, deamangrove all correlated
and mmpexposure and mmpfrequency
and meanwspeed and wspeed_u

```{r}
# plotting relationship of each predictor with response to inform var.monotone
data_join%>%arrange(ruggedness1)%>%
  mutate(group=ceiling(row_number()/100))%>%
  group_by(group)%>%
  summarise(mean_binom=mean(data_type), 
            mean_var=mean(ruggedness1))%>%
  ggplot()+
  geom_point(aes(y=mean_binom, x=mean_var))
```
dist2recboatfeat- 0
dist2rivers- -1
bathy- -1
dist2coast- -1
dist2reefs- -1
seagrass- 0
tidalexposure-0 
mmpexposure-0
mmpfrequency 0
salt 0
temp 0
mean_cur 0
u 0
v 0
mean_wspped 0
wspeed_u -1
wspeed_v -1
Zool_N 0
EFI 0
Secchi 0
deamangroves 0
slope -1
ruggedness -1

```{r}
# plotting relationship of each iredictor with response to inform var.monotone
data_join_b%>%arrange(dist2recboatfeat)%>%
  mutate(group=ceiling(row_number()/100))%>%
  group_by(group)%>%
  summarise(mean_binom=mean(dist2recboatfeat), 
            mean_var=mean(slope1))%>%
  ggplot()+
  geom_point(aes(y=mean_binom, x=mean_var))
```

dist2recboatfeat- 1
dist2rivers- 1
bathy- -1
dist2coast- 0
dist2reefs- 0
seagrass- 0
tidalexposure-0 
mmpexposure-0
mmpfrequency 0
salt 0
temp 0
mean_cur 0
u 0
v 0
mean_wspped 1
wspeed_u -1
wspeed_v -1
Zool_N 1
EFI 1
Secchi 1
deamangroves 1
slope 0
ruggedness 0

# model runs
```{r}

try(BRT.crw<-gbm.fixed(data=train_data, 
                       gbm.x=c(6:9, 11:19, 22, 25:27, 30:34), 
                       gbm.y=3, 
                       family="bernoulli",
                       var.monotone=c(0,0,-1,-1,-1,-1,0,0,0,0,0,-1,-1,0,0,0,0,0,0,0,0,0), # dist 2 coast has max values at coast
                       tree.complexity=5, 
                       learning.rate = 0.005, 
                       n.trees = 9000, 
                       bag.fraction=0.75)) 


#brtgl?
try(BRT.crwGL<-gbm.fixed(data=train_dataGL, 
                       gbm.x=c(6:9, 11:19, 22, 25:27, 30:34), 
                       gbm.y=3, 
                       family="bernoulli",
                       var.monotone=c(0,0,-1,-1,-1,-1,0,0,0,0,0,-1,-1,0,0,0,0,0,0,0,0,0),
                       tree.complexity=5, 
                       learning.rate = 0.005, 
                       n.trees = 9000, 
                       bag.fraction=0.75)) 
#brtunmodified
try(BRT.crwunmod<-gbm.fixed(data=train_dataunmodified, 
                       gbm.x=c(6:9, 11:19, 22, 25:27, 30:34), 
                       gbm.y=3, 
                       family="bernoulli",
                       var.monotone=c(0,0,-1,-1,-1,-1,0,0,0,0,0,-1,-1,0,0,0,0,0,0,0,0,0),
                       tree.complexity=5, 
                       learning.rate = 0.005, 
                       n.trees = 9000, 
                       bag.fraction=0.75))
saveRDS(BRT.crw, "BRT.crw.Rds")
saveRDS(BRT.crwGL, "BRT.crwGL.Rds")
saveRDS(BRT.crwunmod, "BRT.crwunmod.Rds")

```
```{r}
# models with background sampling
try(BRT.background1<-gbm.fixed(data=as.data.frame(train_data_b), 
                       gbm.x=c(6:9, 11:19, 22, 25:27, 30:34), 
                       gbm.y=3, 
                       family="bernoulli",
                       var.monotone=
                         c(0,0,-1,-1,0,0,-1,0,0,0,0,0,0,1,0,0,0,1,1,1,1,0), 
                       tree.complexity=5, 
                       learning.rate = 0.005, 
                       n.trees = 2000, 
                       bag.fraction=0.75)) 


#brtgl?
try(BRT.backgroundGL<-gbm.fixed(data=as.data.frame(train_dataGL_b), 
                       gbm.x=c(6:9, 11:19, 22, 25:27, 30:34), 
                       gbm.y=3, 
                       family="bernoulli",
                       var.monotone=
                         c(0,0,-1,-1,0,0,-1,0,0,0,0,0,0,1,0,0,0,1,1,1,1,0), 
                       tree.complexity=5, 
                       learning.rate = 0.005, 
                       n.trees = 2000, 
                       bag.fraction=0.75)) 
#brtunmodified
try(BRT.backgroundunmod<-gbm.fixed(data=as.data.frame(train_dataunmodified_b), 
                       gbm.x=c(6:9, 11:19, 22, 25:27, 30:34), 
                       gbm.y=3,  
                       family="bernoulli",
                       var.monotone=
                         c(0,0,-1,-1,0,0,-1,0,0,0,0,0,0,1,0,0,0,1,1,1,1,0), 
                       tree.complexity=5, 
                       learning.rate = 0.005, 
                       n.trees = 2000, 
                       bag.fraction=0.75))
saveRDS(BRT.background1, "BRT.background1.Rds")
saveRDS(BRT.backgroundGL, "BRT.backgroundGL.Rds")
saveRDS(BRT.backgroundunmod, "BRT.backgroundunmod.Rds")
```




## load pre-run mods 
```{r}
BRT.crw<-readRDS("BRT.crw.Rds")
BRT.crwGL<-readRDS("BRT.crwGL.Rds")
BRT.crwunmod<-readRDS("BRT.crwunmod.Rds")


BRT.background1<-readRDS("BRT.background1.Rds")
BRT.backgroundGL<-readRDS("BRT.backgroundGL.Rds")
BRT.backgroundunmod<-readRDS("BRT.backgroundunmod.Rds")
```

##########################
# re-run with 5-fold CV
```{r}
library(pROC)

# Set up 5-fold CV
set.seed(42)
folds <- cut(seq(1, nrow(unmodified)), breaks = 5, labels = FALSE)

all_importances <- list()
auc_scores <- c()
best_iter_list <- c() 

for (k in 1:5) {
  test_indices <- which(folds == k, arr.ind = TRUE)
  test_fold <- unmodified[test_indices, ]
  train_fold <- unmodified[-test_indices, ]
  
  # Fit model
  model_k <- gbm.fixed(
    data = train_fold,
    gbm.x = c(6:9, 11:19, 22, 25:27, 30:34),
    gbm.y = 3,
    family = "bernoulli",
    var.monotone = c(0,0,-1,-1,0,0,-1,0,0,0,0,0,0,1,0,0,0,1,1,1,1,0),
    tree.complexity = 5,
    learning.rate = 0.005,
    n.trees = 10000,
    bag.fraction = 0.75
  )
  
    # Determine best number of trees via OOB deviance
  best_trees <- gbm.perf(model_k, method = "OOB", plot.it = FALSE)
  best_iter_list[k] <- best_trees
  cat("  Best iteration (trees):", best_trees, "\n")
  
  # Predict on held-out fold
  preds <- predict.gbm(
    model_k,
    newdata = test_fold,
    type = "response",
    n.trees = best_trees
  )
  
  # Evaluate AUC
  roc_obj <- roc(test_fold[, 3], preds)
  auc_scores[k] <- auc(roc_obj)
  
  # variable importance
  imp <- summary(model_k, plotit = FALSE)
  imp$Fold <- k
  all_importances[[k]] <- imp
}

mean_auc <- mean(auc_scores)
print(paste("Mean AUC across folds:", round(mean_auc, 3)))
```
```{r}
# Combine importance data across folds
imp_df <- do.call(rbind, all_importances)

# Calculate mean importance per variable
library(dplyr)
avg_importance <- imp_df %>%
  group_by(var) %>%
  summarise(mean_rel_inf = mean(rel.inf)) %>%
  arrange(mean_rel_inf)

# Optional: print top and bottom variables
print("Lowest importance predictors:")
print(head(avg_importance, 5))

# Identify lowest 20% to consider removing
n_vars <- nrow(avg_importance)
low_vars <- avg_importance$var[1:ceiling(0.2 * n_vars)]
```
```{r}
vars_to_drop <- c("geohab", "geomorph", "mean_wspeed", "EFI", "Secchi") 
selected_vars <- names(train_dataunmodified)[c(6:9, 11:19, 22, 25:27, 30:34)]
final_vars <- setdiff(selected_vars, vars_to_drop)
gbm_x_indices <- which(names(train_dataunmodified) %in% final_vars)
```


Rerun CV, removing vars that are consistently unimportant and changing n.trees to match best.iter of folds from previous run.
```{r}
# Set up 5-fold CV
set.seed(42)
folds <- cut(seq(1, nrow(unmodified)), breaks = 5, labels = FALSE)

all_importances <- list()
auc_scores <- c()
best_iter_list <- c() 

for (k in 1:5) {
  test_indices <- which(folds == k, arr.ind = TRUE)
  test_fold <- unmodified[test_indices, ]
  train_fold <- unmodified[-test_indices, ]

  # Fit model
  model_k <- gbm.fixed(
    data = train_fold,
    gbm.x = gbm_x_indices,
    gbm.y = 3,
    family = "bernoulli",
    var.monotone = c(-1,-1,0,0,-1,0,0,0,0,0,0,1,0,0,0,1,0),
    tree.complexity = 5,
    learning.rate = 0.005,
    n.trees = 3000,
    bag.fraction = 0.75
  )
  
    # Determine best number of trees via OOB deviance
  best_trees <- gbm.perf(model_k, method = "OOB", plot.it = FALSE)
  best_iter_list[k] <- best_trees
  cat("  Best iteration (trees):", best_trees, "\n")
  
  # Predict on held-out fold
  preds <- predict.gbm(
    model_k,
    newdata = test_fold,
    type = "response",
    n.trees = best_trees
  )
  
  # Evaluate AUC
  roc_obj <- roc(test_fold[, 3], preds)
  auc_scores[k] <- auc(roc_obj)
  
  # variable importance
  imp <- summary(model_k, plotit = FALSE)
  imp$Fold <- k
  all_importances[[k]] <- imp
}

mean_auc <- mean(auc_scores)
print(paste("Mean AUC across folds:", round(mean_auc, 3)))
```
If drop in AUC is >0.02, check other model performance metrics.
```{r}
# Combine importance data across folds
imp_df <- do.call(rbind, all_importances)

# Calculate mean importance per variable
avg_importance <- imp_df %>%
  group_by(var) %>%
  summarise(mean_rel_inf = mean(rel.inf)) %>%
  arrange(mean_rel_inf)

# Optional: print top and bottom variables
print("Lowest importance predictors:")
print(head(avg_importance, 5))

# Identify lowest 20% to consider removing
n_vars <- nrow(avg_importance)
low_vars <- avg_importance$var[1:ceiling(0.2 * n_vars)]
```
# consider removing and rerunning CV? Zool_N, seagrassP1, slope1, COMMUNITY, bathy1

```{r}
vars_to_drop <- c("ZooL_N", "seagrassP1", "slope1", "COMMUNITY", "bathy1", "geohab", "geomorph", "mean_wspeed", "EFI", "Secchi") 
selected_vars <- names(train_dataunmodified)[c(6:9, 11:19, 22, 25:27, 30:34)]
final_vars <- setdiff(selected_vars, vars_to_drop)
gbm_x_indices <- which(names(train_dataunmodified) %in% final_vars)
```

```{r}
# Set up 5-fold CV
set.seed(42)
folds <- cut(seq(1, nrow(unmodified)), breaks = 5, labels = FALSE)

all_importances <- list()
auc_scores <- c()
best_iter_list <- c() 

for (k in 1:5) {
  test_indices <- which(folds == k, arr.ind = TRUE)
  test_fold <- unmodified[test_indices, ]
  train_fold <- unmodified[-test_indices, ]

  # Fit model
  model_k <- gbm.fixed(
    data = train_fold,
    gbm.x = gbm_x_indices,
    gbm.y = 3,
    family = "bernoulli",
    var.monotone = c(-1,0,0,-1,0,0,0,0,1,0,0,0),
    tree.complexity = 5,
    learning.rate = 0.005,
    n.trees = 3000,
    bag.fraction = 0.75
  )
  
    # Determine best number of trees via OOB deviance
  best_trees <- gbm.perf(model_k, method = "OOB", plot.it = FALSE)
  best_iter_list[k] <- best_trees
  cat("  Best iteration (trees):", best_trees, "\n")
  
  # Predict on held-out fold
  preds <- predict.gbm(
    model_k,
    newdata = test_fold,
    type = "response",
    n.trees = best_trees
  )
  
  # Evaluate AUC
  roc_obj <- roc(test_fold[, 3], preds)
  auc_scores[k] <- auc(roc_obj)
  
  # variable importance
  imp <- summary(model_k, plotit = FALSE)
  imp$Fold <- k
  all_importances[[k]] <- imp
}

mean_auc <- mean(auc_scores)
print(paste("Mean AUC across folds:", round(mean_auc, 3)))
```
AUC has actually gone up


Final model run, using params from CV
```{r}
final_modelunmod <- gbm.fixed(
  data = unmodified,
  gbm.x = gbm_x_indices,   # cleaned feature set
  gbm.y = 3,
  family = "bernoulli",
  var.monotone = c(-1,0,0,-1,0,0,0,0,1,0,0,0),
  tree.complexity = 5,
  learning.rate = 0.005,
  n.trees = 3000,  
  bag.fraction = 0.75
)
saveRDS(final_modelunmod, "final_unmod.Rds")
```

```{r}
# pruning
(best.iter=gbm.perf(final_modelunmod, method="OOB"))
summary(final_modelunmod)
```

##### repeat model formulaiton processfor GL
whole model with CV
```{r}

# Set up 5-fold CV
set.seed(42)
folds <- cut(seq(1, nrow(GL)), breaks = 5, labels = FALSE)

all_importances <- list()
auc_scores <- c()
best_iter_list <- c() 

for (k in 1:5) {
  test_indices <- which(folds == k, arr.ind = TRUE)
  test_fold <- GL[test_indices, ]
  train_fold <- GL[-test_indices, ]
  
  # Fit model
  model_k <- gbm.fixed(
    data = train_fold,
    gbm.x = c(6:9, 11:19, 22, 25:27, 30:34),
    gbm.y = 3,
    family = "bernoulli",
    var.monotone = c(0,0,-1,-1,0,0,-1,0,0,0,0,0,0,1,0,0,0,1,1,1,1,0),
    tree.complexity = 5,
    learning.rate = 0.005,
    n.trees = 5000,
    bag.fraction = 0.75
  )
  
    # Determine best number of trees via OOB deviance
  best_trees <- gbm.perf(model_k, method = "OOB", plot.it = FALSE)
  best_iter_list[k] <- best_trees
  cat("  Best iteration (trees):", best_trees, "\n")
  
  # Predict on held-out fold
  preds <- predict.gbm(
    model_k,
    newdata = test_fold,
    type = "response",
    n.trees = best_trees
  )
  
  # Evaluate AUC
  roc_obj <- roc(test_fold[, 3], preds)
  auc_scores[k] <- auc(roc_obj)
  
  # variable importance
  imp <- summary(model_k, plotit = FALSE)
  imp$Fold <- k
  all_importances[[k]] <- imp
}

mean_auc <- mean(auc_scores)
print(paste("Mean AUC across folds:", round(mean_auc, 3)))
```


```{r}
# Combine importance data across folds
imp_df <- do.call(rbind, all_importances)

# Calculate mean importance per variable
library(dplyr)
avg_importance <- imp_df %>%
  group_by(var) %>%
  summarise(mean_rel_inf = mean(rel.inf)) %>%
  arrange(mean_rel_inf)

# Optional: print top and bottom variables
print("Lowest importance predictors:")
print(head(avg_importance, 15))

# Identify lowest 20% to consider removing
n_vars <- nrow(avg_importance)
low_vars <- avg_importance$var[1:ceiling(0.2 * n_vars)]
```
```{r}
vars_to_drop <- c('geomorph', 'bathy1', 'slope1', 'EFI', 'mean_cur', 'mean_wspeed', 'Secchi', 'ZooL_N', 'ruggedness1', 'mmpexposure1') # update this!
selected_vars <- names(GL)[c(6:9, 11:19, 22, 25:27, 30:34)]
final_vars <- setdiff(selected_vars, vars_to_drop)
gbm_x_indices <- which(names(GL) %in% final_vars)
```


Rerun CV, removing vars that are consistently unimportant and changing n.trees to match best.iter of folds from previous run.
```{r}
# Set up 5-fold CV
set.seed(42)
folds <- cut(seq(1, nrow(GL)), breaks = 5, labels = FALSE)

all_importances <- list()
auc_scores <- c()
best_iter_list <- c() 

for (k in 1:5) {
  test_indices <- which(folds == k, arr.ind = TRUE)
  test_fold <- GL[test_indices, ]
  train_fold <- GL[-test_indices, ]

  # Fit model
  model_k <- gbm.fixed(
    data = train_fold,
    gbm.x = gbm_x_indices,
    gbm.y = 3,
    family = "bernoulli",
    var.monotone = c(0,-1,-1,-1,0,0,0,0,0,0,0,0), # update this!
    tree.complexity = 5,
    learning.rate = 0.005,
    n.trees = 10000,
    bag.fraction = 0.75
  )
  
    # Determine best number of trees via OOB deviance
  best_trees <- gbm.perf(model_k, method = "OOB", plot.it = FALSE)
  best_iter_list[k] <- best_trees
  cat("  Best iteration (trees):", best_trees, "\n")
  
  # Predict on held-out fold
  preds <- predict.gbm(
    model_k,
    newdata = test_fold,
    type = "response",
    n.trees = best_trees
  )
  
  # Evaluate AUC
  roc_obj <- roc(test_fold[, 3], preds)
  auc_scores[k] <- auc(roc_obj)
  
  # variable importance
  imp <- summary(model_k, plotit = FALSE)
  imp$Fold <- k
  all_importances[[k]] <- imp
}

mean_auc <- mean(auc_scores)
print(paste("Mean AUC across folds:", round(mean_auc, 3)))
```
```{r}
# Combine importance data across folds
imp_df <- do.call(rbind, all_importances)

# Calculate mean importance per variable
library(dplyr)
avg_importance <- imp_df %>%
  group_by(var) %>%
  summarise(mean_rel_inf = mean(rel.inf)) %>%
  arrange(mean_rel_inf)

# Optional: print top and bottom variables
print("Lowest importance predictors:")
print(head(avg_importance, 15))

# Identify lowest 20% to consider removing
n_vars <- nrow(avg_importance)
low_vars <- avg_importance$var[1:ceiling(0.2 * n_vars)]
```
```{r}
vars_to_drop <- c('dist2coast1_2', 'geohab', 'geomorph', 'bathy1', 'slope1', 'EFI', 'mean_cur', 'mean_wspeed', 'Secchi', 'ZooL_N', 'ruggedness1', 'mmpexposure1') # update this!
selected_vars <- names(GL)[c(6:9, 11:19, 22, 25:27, 30:34)]
final_vars <- setdiff(selected_vars, vars_to_drop)
gbm_x_indices <- which(names(GL) %in% final_vars)
```


Rerun CV, removing vars that are consistently unimportant and changing n.trees to match best.iter of folds from previous run.
```{r}
# Set up 5-fold CV
set.seed(42)
folds <- cut(seq(1, nrow(GL)), breaks = 5, labels = FALSE)

all_importances <- list()
auc_scores <- c()
best_iter_list <- c() 

for (k in 1:5) {
  test_indices <- which(folds == k, arr.ind = TRUE)
  test_fold <- GL[test_indices, ]
  train_fold <- GL[-test_indices, ]

  # Fit model
  model_k <- gbm.fixed(
    data = train_fold,
    gbm.x = gbm_x_indices,
    gbm.y = 3,
    family = "bernoulli",
    var.monotone = c(-1,0,-1,0,0,0,1,0,0,0)), # update this!
    tree.complexity = 5,
    learning.rate = 0.005,
    n.trees = 15000,
    bag.fraction = 0.75
  )
  
    # Determine best number of trees via OOB deviance
  best_trees <- gbm.perf(model_k, method = "OOB", plot.it = FALSE)
  best_iter_list[k] <- best_trees
  cat("  Best iteration (trees):", best_trees, "\n")
  
  # Predict on held-out fold
  preds <- predict.gbm(
    model_k,
    newdata = test_fold,
    type = "response",
    n.trees = best_trees
  )
  
  # Evaluate AUC
  roc_obj <- roc(test_fold[, 3], preds)
  auc_scores[k] <- auc(roc_obj)
  
  # variable importance
  imp <- summary(model_k, plotit = FALSE)
  imp$Fold <- k
  all_importances[[k]] <- imp
}

mean_auc <- mean(auc_scores)
print(paste("Mean AUC across folds:", round(mean_auc, 3)))
```

Compute final model with new params
```{r}
final_modelGL <- gbm.fixed(
  data = GL,
  gbm.x = gbm_x_indices,   # cleaned feature set
  gbm.y = 3,
  family = "bernoulli",
  var.monotone = c(-1,0,-1,0,0,0,1,0,0,0), # update
  tree.complexity = 5,
  learning.rate = 0.005,
  n.trees = 20000,  # update
  bag.fraction = 0.75
)

(best.iter=gbm.perf(final_modelGL, method="OOB"))
saveRDS(final_modelGL, "interim_GL.Rds")
```

Run with final configuration
```{r}
vars_to_drop <- c('dist2coast1_2', 'geohab', 'geomorph', 'bathy1', 'slope1', 'EFI', 'mean_cur', 'mean_wspeed', 'Secchi', 'ZooL_N', 'ruggedness1', 'mmpexposure1') # update this!
selected_vars <- names(GL)[c(6:9, 11:19, 22, 25:27, 30:34)]
final_vars <- setdiff(selected_vars, vars_to_drop)
gbm_x_indices <- which(names(GL) %in% final_vars)

set.seed(42)

final_modelGL <- gbm.fixed(
  data = GL,
  gbm.x = gbm_x_indices,   # cleaned feature set
  gbm.y = 3,
  family = "bernoulli",
  var.monotone = c(-1,0,-1,0,0,0,1,0,0,0), # update
  tree.complexity = 5,
  learning.rate = 0.005,
  n.trees = 20000,  # update
  bag.fraction = 0.75
)
saveRDS(final_modelGL, "final_modelGL.Rds")
```


################

load models
```{r}
final_modelunmod<-readRDS("final_unmod.Rds")
final_modelGL<-readRDS("final_modelGL.Rds")
#final_modelALL<-readRDS("final_modelALL.Rds")

(best.iter=gbm.perf(final_modelunmod, method="OOB"))
(best.iterGL=gbm.perf(final_modelGL, method="OOB"))
#(best.iterALL=gbm.perf(final_modelALL, method="OOB"))
```


```{r}
# get a list of predictors in alphabetical order
a<-summary(final_modelunmod, n.trees=best.iter)$var
sorted<-sort(a)
sorted
```


```{r}
unmodc<-summary(final_modelunmod, n.trees=best.iter)
GLc<-summary(final_modelGL, n.trees=best.iterGL)
# ALLc<-summary(final_modelALL, n.trees=best.iterALL)

combined_df<-reduce(list(unmodc, GLc), full_join, by="var")
b.df<-combined_df%>%
  rename(unmodified=rel.inf.x,
         modified=rel.inf.y)%>%arrange(tolower(var))
b.df

# rename stuff for plotting
names(b.df)[names(b.df)=="var"]<-"Variable"
b.df$"Variable"<-c("Distance to Reefs",
                   "Means seawater velocity", 
                   "Distance to boat ramps", 
                   "Acute flood frequency", 
                   "Salinity", 
                   "Tidal exposure", 
                   "Distance to coast",
                   "Distance to mangroves", 
                   "Distance to rivers", 
                   "Ruggedness", 
                   "Temperature", 
                   "Chronic floodwater exposure", 
                   "Seagrass community type", 
                   "Seagrass probability")
```


Predict on the full dataset

```{r}
#source("PA-paper/ModelFunctions.R")
#source("PA-paper/Model_Eval_Fcns.R")
predunmod <- predict.gbm(
  final_modelunmod,
  newdata = unmodified,
  type = "response",
  n.trees = best.iter
)
predGL <- predict.gbm(
  final_modelGL,
  newdata = GL,
  type = "response",
  n.trees = best.iterGL
)
```
https://besjournals.onlinelibrary.wiley.com/doi/10.1111/j.1365-2656.2008.01390.x#b18 
https://rspatial.org/raster/sdm/6_sdm_methods.html

Unmod diagnostics
```{r}
#AUC and TSS (true skill statistic)- predictive skill
obs<- unmodified$data_type
Evaluations_full <- as.data.frame(matrix(data=0,nrow=1,ncol=3))
  colnames(Evaluations_full) <- c("AUC","TSS","TPR")
#dev <- calc.deviance(obs=test_data$data_type, pred=pred, calc.mean=TRUE)
#roc<-roc(test_data$data_type, pred)
  d <- cbind(obs, predunmod)
  pres <- d[d[,1]==1,2]
  abs <- d[d[,1]==0,2]
  e <- evaluate(p=pres, a=abs)
  Evaluations_full[1,1] <- e@auc
  Evaluations_full[1,2] <- max(e@TPR + e@TNR-1)
  Evaluations_full[1,3] <- mean(e@TPR)
  
  
  
(Evaluations_full)
eval<-c(names(Evaluations_full), "% deviance explained")
b.df[15:18,1]<-eval
b.df[15,2]<-Evaluations_full[,1]
b.df[16,2]<-Evaluations_full[,2]
b.df[17,2]<-Evaluations_full[,3]
```
AUC of 0.5 means the model is just as good as a random guess. AUC varies with spatial extent used to select background points. Larger extent, higher AUC. 

# Deviance explained - explanatory power
```{r}
x<-(final_modelunmod$self.statistics$null.deviance-final_modelunmod$self.statistics$resid.deviance)/final_modelunmod$self.statistics$null.deviance
b.df[18,2]<-x
```

GL diagnostics
```{r}
#AUC and TSS (true skill statistic)- predictive skill

Evaluations_fullGL <- as.data.frame(matrix(data=0,nrow=1,ncol=3))
  colnames(Evaluations_fullGL) <- c("AUC","TSS","TPR")
#dev <- calc.deviance(obs=test_data$data_type, pred=pred, calc.mean=TRUE)
#roc<-roc(test_data$data_type, pred)
  d <- cbind(GL$data_type, predGL)
  pres <- d[d[,1]==1,2]
  abs <- d[d[,1]==0,2]
  e <- evaluate(p=pres, a=abs)
  Evaluations_fullGL[1,1] <- e@auc
  Evaluations_fullGL[1,2] <- max(e@TPR + e@TNR-1)
  Evaluations_fullGL[1,3] <- mean(e@TPR)
  
(Evaluations_fullGL)
b.df[15,3]<-Evaluations_fullGL[,1]
b.df[16,3]<-Evaluations_fullGL[,2]
b.df[17,3]<-Evaluations_fullGL[,3]
  
Evaluations_fullunmod <- as.data.frame(matrix(data=0,nrow=1,ncol=3))
  colnames(Evaluations_fullunmod) <- c("AUC","TSS","TPR")
```
```{r}
#GL
y<-(final_modelGL$self.statistics$null.deviance-final_modelGL$self.statistics$resid.deviance)/final_modelGL$self.statistics$null.deviance
b.df[18,3]<-y
```


# Create a summary table w/heatmap for rel.inf
```{r}
library(gt)
library(webshot2)
threshold1<-100/12
threshold2<-100/10

# Ensure columns are numeric and handle NAs
b.df[, c("unmodified", "modified")] <- 
  lapply(b.df[, c("unmodified", "modified")], 
         function(x) as.numeric(x))

gt_table <- b.df %>% 
  gt() %>%
  # Color the cells based on their values, excluding the last 4 rows
  data_color(
    columns = c("unmodified", "modified"),
    rows = 1:(nrow(b.df)-4), # Exclude the last 4 rows
        fn = function(x) {
      # Use scales::col_numeric to create a color function
      scales::col_numeric(
        palette = c("blue", "yellow", "red"),
        domain = range(b.df[,2:3], na.rm = TRUE)
      )(x)
    }
  ) %>%
  fmt_number(
    columns = c("unmodified", "modified"),
    decimals = 2 # Format numbers to two decimal places
  ) 
  
  gt_table <- gt_table %>%
  tab_style(
    style = cell_borders(
      sides = "all",
      color = "black",
      weight = px(3)
    ),
    locations = list(
        cells_body(
          columns = unmodified, 
          rows = unmodified>threshold1
        ),
         cells_body(
          columns = modified, 
          rows = modified>threshold2
        )
  )
)%>%tab_spanner_delim(
    delim = "_"
)
gt_table
gtsave(gt_table, filename="rel.importance.png", expand=12)
```
# Legend
```{r}
legend_data <- data.frame(
  Value = seq(min(b.df[,2:4]), max(b.df[,2:4]), length.out = 100)
)

# Create the legend as a plot
legend_plot <- ggplot(legend_data, aes(x = Value, y = 1, fill = Value)) +
  geom_tile() +
  scale_fill_gradientn(colors = c("blue", "yellow", "red")) +
  theme_void() +
  theme(legend.position = "bottom", legend.title=element_blank(), legend.direction="vertical", legend.text=element_text(size=14)) 

# Plot the legend
library(cowplot)
legend<-get_legend(legend_plot)
p<-plot(legend)
ggsave(legend, filename="relimportance_legend.png", width=1, height=2)
```


# Explore partial effects (partial dependence plots)
i.e. what would the effect of the predictor be if you averaged out the effects of all the other predictors in the full model

yhat
- in logistic regression, is the log-odss of the probability of the outcome (presence) being 1.
- can transform to probability = 1/(1+exp(-yhat)) to give the probability of each observation being a presence.
- The y-axis of a partial dependence plot for regression represents the marginal impact of the independent variable to the dependent variable. E.g. if the line is at 0, then for that value of the independent variable, there is 0 impact to the dependent variable.


```{r}
nms <- colnames(unmodified)
r <- vector('list', 5)
new_x_labels <- c("Distance to reefs", "Mean seawater velocity", "Distance to boat ramps", "Acute flood frequency", "Salinity")
names(r) <- nms[c(11,27,13,16,25)] #change these according to model rank list
for (j in seq_along(nms[c(11,27,13,16,25)])) {
  nm <- nms[c(11,27,13,16,25)][j]
  print(nm)
  r[[nm]] <- final_modelunmod %>% pdp::partial(pred.var=nm,
                                 n.trees=best.iter,
                                 train=unmodified,
                                 #inv.link=plogis,
                                 #recursive=FALSE,
                                 type='regression') %>%
    autoplot()+theme_classic() +# + ylim(0, 1)

    labs(x=new_x_labels[j])
  if (nm == "COMMUNITY") { # Rotate x-axis tick labels in the first panel
    r[[nm]] <- r[[nm]] + theme(axis.text.x = element_text(angle = 90, hjust = 1))+
                                 scale_x_discrete(labels = function(x) {
        x[x == "Non-seagrass habitat"] <- "NSG"
        return(x)
      })
  }
}
 
plot<-do.call('grid.arrange', r)
library(forestplot)
title <- textGrob("A- CRW, unmodified", x = unit(0, "npc"), just = "left", gp = gpar(fontsize = 16))

# Combine the title and the plot
final_plot <- arrangeGrob(plot, top = title)
final_plot
ggsave("CRWunmod.partial.png", final_plot, width=10, height=8)
 

# 
# # which categories best predict presence
# pdp_df <- final_modelunmod %>% pdp::partial(pred.var='COMMUNITY',
#                                  n.trees=best.iterGL,
#                                  train=train_data,
#                                  # inv.link=plogis,
#                                  # recursive=FALSE,
#                                  type='regression')
# 
# # Now, sort the data frame by the predicted probabilities (assuming 'y' column contains these)
# sorted_pdp <- pdp_df[order(pdp_df$y, decreasing = TRUE), ]
# 
# # If you want to view the sorted categories alongside their probabilities
# print(sorted_pdp)
```

```{r}
s <- vector('list', 5)
names(s) <- nms[c(13,26,11,25,34)] #change these according to model rank list
new_x_labels <- c("Distance to boat ramps", "Temperature", "Distance to reefs", "Salinity", "Seagrass community type")
for (j in seq_along(nms[c(13,26,11,25,34)])) {
  nm <- nms[c(13,26,11,25,34)][j]
  print(nm)
  s[[nm]] <- final_modelGL %>% pdp::partial(pred.var=nm,
                                 n.trees=best.iterGL,
                                 train=GL,
                                 #inv.link=plogis,
                                 #recursive=FALSE,
                                 type='regression') %>%
    autoplot()+theme_classic() +# + ylim(0, 1)

    labs(x=new_x_labels[j])
  if (nm == "COMMUNITY") { # Rotate x-axis tick labels in the first panel
    s[[nm]] <- s[[nm]] + theme(axis.text.x = element_text(angle = 90, hjust = 1))+
                                 scale_x_discrete(labels = function(x) {
        x[x == "Non-seagrass habitat"] <- "NSG"
        return(x)
      })
  }
}
 
plot<-do.call('grid.arrange', s)

title <- textGrob("D- CRW, modified", x = unit(0, "npc"), just = "left", gp = gpar(fontsize = 16))

# Combine the title and the plot
final_plot <- arrangeGrob(plot, top = title)
final_plot
ggsave("CRWGL.partial.png", final_plot, width=8, height=6)
# # which categories best predict presence
pdp_df <- final_modelGL %>% pdp::partial(pred.var='COMMUNITY',
                                 n.trees=best.iterGL,
                                 train=GL,
                                 # inv.link=plogis,
                                 # recursive=FALSE,
                                 type='regression')

# Now, sort the data frame by the predicted probabilities (assuming 'y' column contains these)
sorted_pdp <- pdp_df[order(pdp_df$y, decreasing = TRUE), ]

# If you want to view the sorted categories alongside their probabilities
print(sorted_pdp)
```






Environmental distance
```{r}
bhattacharyya_distance <- function(mu1, sigma1, mu2, sigma2) {
  # Calculate the average variance
  sigma_avg <- (sigma1 + sigma2) / 2
  
  # Calculate the Bhattacharyya distance
  D_B <- 1/4 * log(1/4 * (sigma1/sigma2 + sigma2/sigma1 + 2)) + 1/4 * ((mu1 - mu2)^2 / sigma_avg)
  
  return(D_B)
}
```


### crw
```{r}
track_filtered<-data_join%>%filter(data_type==1)
crw_filtered<-data_join%>%filter(data_type==0)
mu1<-mean(track_filtered$dist2recboatfeat, na.rm=T)
sigma1<-var(track_filtered$dist2recboatfeat, na.rm=T)
mu2<-mean(crw_filtered$dist2recboatfeat, na.rm=T)
sigma2<-var(crw_filtered$dist2recboatfeat, na.rm=T)
# coefficient
sqrt(2 * sigma1 * sigma2 / (sigma1^2 + sigma2^2)) * exp(-1/4 * ((mu1 - mu2)^2 / (sigma1^2 + sigma2^2)))


mu1<-mean(track_filtered$salt, na.rm=T)
sigma1<-var(track_filtered$salt, na.rm=T)
mu2<-mean(crw_filtered$salt, na.rm=T)
sigma2<-var(crw_filtered$salt, na.rm=T)
# coefficient
sqrt(2 * sigma1 * sigma2 / (sigma1^2 + sigma2^2)) * exp(-1/4 * ((mu1 - mu2)^2 / (sigma1^2 + sigma2^2)))


mu1<-mean(track_filtered$mean_cur, na.rm=T)
sigma1<-var(track_filtered$mean_cur, na.rm=T)
mu2<-mean(crw_filtered$mean_cur, na.rm=T)
sigma2<-var(crw_filtered$mean_cur, na.rm=T)
# coefficient
sqrt(2 * sigma1 * sigma2 / (sigma1^2 + sigma2^2)) * exp(-1/4 * ((mu1 - mu2)^2 / (sigma1^2 + sigma2^2)))

mu1<-mean(track_filtered$mmpfrequency1, na.rm=T)
sigma1<-var(track_filtered$mmpfrequency1, na.rm=T)
mu2<-mean(crw_filtered$mmpfrequency1, na.rm=T)
sigma2<-var(crw_filtered$mmpfrequency1, na.rm=T)
# coefficient
sqrt(2 * sigma1 * sigma2 / (sigma1^2 + sigma2^2)) * exp(-1/4 * ((mu1 - mu2)^2 / (sigma1^2 + sigma2^2)))

mu1<-mean(track_filtered$dist2reefs1, na.rm=T)
sigma1<-var(track_filtered$dist2reefs1, na.rm=T)
mu2<-mean(crw_filtered$dist2reefs1, na.rm=T)
sigma2<-var(crw_filtered$dist2reefs1, na.rm=T)
# coefficient
sqrt(2 * sigma1 * sigma2 / (sigma1^2 + sigma2^2)) * exp(-1/4 * ((mu1 - mu2)^2 / (sigma1^2 + sigma2^2)))

mu1<-mean(track_filtered$temp, na.rm=T)
sigma1<-var(track_filtered$temp, na.rm=T)
mu2<-mean(crw_filtered$temp, na.rm=T)
sigma2<-var(crw_filtered$temp, na.rm=T)
# coefficient
sqrt(2 * sigma1 * sigma2 / (sigma1^2 + sigma2^2)) * exp(-1/4 * ((mu1 - mu2)^2 / (sigma1^2 + sigma2^2)))

# 
# #### categorical
# 
# levels <- union(track_filtered$COMMUNITY, crw_filtered$COMMUNITY)
# sample1_factor <- factor(track_filtered$COMMUNITY, levels = levels)
# sample2_factor <- factor(crw_filtered$COMMUNITY, levels = levels)
# 
# # Calculate probabilities for each category in both samples
# prob1 <- table(sample1_factor) / length(sample1_factor)
# prob2 <- table(sample2_factor) / length(sample2_factor)
# 
# # Calculate the Bhattacharyya coefficient
# BC <- sum(sqrt(prob1 * prob2))
```



# interactions?
int.size represents the number of times an interaction between the 2 variables is used in the decision trees (i.e. split was made considering both variables together)
```{r}
names(final_modelunmod$gbm.call)[1]<-"dataframe"
find.int<-gbm.interactions(final_modelunmod)
find.int
find.int$rank.list
```
```{r}
names(BRT.crwGL$gbm.call)[1]<-"dataframe"
find.int<-gbm.interactions(BRT.crwGL)
find.int
find.int$rank.list
```

```{r}
names(BRT.crwunmod$gbm.call)[1]<-"dataframe"
find.int<-gbm.interactions(BRT.crwunmod)
find.int
find.int$rank.list
```

```{r}
names(final_modelGL$gbm.call)[1]<-"dataframe"
find.intGL<-gbm.interactions(final_modelGL)
find.intGL
find.intGL$rank.list
```

Plot pairwise interactions
```{r}
# gbm.perspec(BRT.crw, 13,1)
# gbm.perspec(BRT.crw, 23, 16)
# gbm.perspec(BRT.crw, 15,5)
# gbm.perspec(BRT.crw, 23,15)
# gbm.perspec(BRT.crw, 23,12)
```
```{r}
# gbm.perspec(BRT.background1, 13,1)
# gbm.perspec(BRT.backgroundGL, 12,5)
# gbm.perspec(BRT.backgroundunmod, 22, 8)
# gbm.perspec(BRT.crw, 15,5)
# gbm.perspec(BRT.crw, 23,15)
# gbm.perspec(BRT.crw, 23,12)
```



# Predicting to grids


```{r}
# 2010 with tif
extract_tif_header_info_raster <- function(file_path) {
  r <- raster(file_path)
  ncols <- ncol(r)
  nrows <- nrow(r)
  cellsize <- res(r)[1] # Assuming uniform resolution in x and y
  
  # Return a list containing the extracted information
  return(list(ncols = ncols, nrows = nrows, cellsize = cellsize))
}


file_paths_tif <- list.files("../enviro datasets/rasters for grid predictions/", pattern = "\\.tif$", full.names = TRUE)
header_info_tif <- lapply(file_paths_tif, extract_tif_header_info_raster)

#make mask the first item in the vector
file_paths_tif<-c(file_paths_tif[18], file_paths_tif[-c(2,3,18,19,26,4,12,14, 37)])
# Identify indices of elements containing "2022"
indices_to_remove <- grep("2022", file_paths_tif)

# Exclude those elements from the vector
file_paths_tif <- file_paths_tif[-indices_to_remove]

# # e.g. Find the column number for "Age"
# column_number <- which(names(df) == "Age")
variable.names2010 <- c("mask", names(unmodified)[c(11,27,13,16,25,15,12,22,9,18,26,17)]) #here make sure the order is the same as above, if you're using different data
processed_layers <- list() # Initialize an empty list to store processed layers
modeled.vars<-file_paths_tif[c(1,6,12,5,15,17,22,4,11,7,16,21,14)]

for(i in 1:length(modeled.vars)) { 
  # Load each .tif file as a RasterLayer object
  r <- raster(modeled.vars[i])
  cat(modeled.vars[i], "\n")
  if (variable.names2010[i] %in% c("COMMUNITY", "geohab", "geomorph", "builtfeature")) {
    r_agg_raster<-r
  } else {
    

  # Convert RasterLayer to SpatRaster for more efficient processing
  r_spat <- terra::rast(r)
  
  # Aggregate the SpatRaster object
  r_agg <- aggregate(r_spat, fact=5, fun=mean, na.rm=TRUE)
  
  # Optionally: Convert back to RasterLayer if needed for compatibility with other raster package functions
  r_agg_raster <- raster(r_agg)
  }
  # Replace nodata value with NA - Note: Adjust according to your data's needs
  currentNodata <- NAvalue(r_agg_raster)
  cat("NoData Value: ", currentNodata, "\n") # all seem to be -Inf
  
  # Store the aggregated RasterLayer object in the list with the desired name
  processed_layers[[variable.names2010[i]]] <- r_agg_raster
  
  # Optionally, assign the RasterLayer object to the global environment with the desired name
  assign(variable.names2010[i], r_agg_raster, envir = .GlobalEnv)
}
```
```{r}
# rasters are too large for memory allocation. Need to aggregate to reduce resolution (to 500m)
```


```{r}
rasters_values <- lapply(variable.names2010, function(name) values(get(name)))
preddat <- data.frame(rasters_values) 
names(preddat) <- variable.names2010
preddat<- preddat[!is.na(preddat$mask),]
```

```{r}
### 2022
file_paths_tif <- list.files("../enviro datasets/rasters for grid predictions/", pattern = "\\.tif$", full.names = TRUE)
#make mask the first item in the vector
file_paths_tif2022<-c(file_paths_tif[18], file_paths_tif[-c(2,3,18,19,26,4,12,14, 37)])
# Identify indices of elements containing "2010"
indices_to_remove <- grep("2010", file_paths_tif2022)

# Exclude those elements from the vector
file_paths_tif2022 <- file_paths_tif2022[-indices_to_remove]

# # e.g. Find the column number for "Age"
# column_number <- which(names(df) == "Age")
variable.names2022 <- c("mask", names(unmodified)[c(11,27,13,16,25,15,12,22,9,18,26,17)]) #here make sure the order is the same as above, if you're using different data
processed_layers2022 <- list() # Initialize an empty list to store processed layers

modeled.vars2022<-file_paths_tif2022[c(1,6,12,5,15,17,22,4,11,7,16,21,14)]

for(i in 1:length(modeled.vars2022)) { 
  # Load each .tif file as a RasterLayer object
  r <- raster(modeled.vars2022[i])
  cat(modeled.vars2022[i], "\n")
  if (variable.names2022[i] %in% c("COMMUNITY", "geohab", "geomorph", "builtfeature")) {
    r_agg_raster<-r
  } else {
    

  # Convert RasterLayer to SpatRaster for more efficient processing
  r_spat <- terra::rast(r)
  
  # Aggregate the SpatRaster object
  r_agg <- aggregate(r_spat, fact=5, fun=mean, na.rm=TRUE)
  
  # Optionally: Convert back to RasterLayer if needed for compatibility with other raster package functions
  r_agg_raster <- raster(r_agg)
  }
  # Replace nodata value with NA - Note: Adjust according to your data's needs
  currentNodata <- NAvalue(r_agg_raster)
  cat("NoData Value: ", currentNodata, "\n") # all seem to be -Inf
  
  # Store the aggregated RasterLayer object in the list with the desired name
  processed_layers2022[[variable.names2022[i]]] <- r_agg_raster
  
  # Optionally, assign the RasterLayer object to the global environment with the desired name
  assign(variable.names2022[i], r_agg_raster, envir = .GlobalEnv)
}
rasters_values2022 <- lapply(variable.names2022, function(name) values(get(name)))
preddat2022 <- data.frame(rasters_values2022) 
names(preddat2022) <- variable.names2022
preddat2022<- preddat2022[!is.na(preddat2022$mask),]
# this step takes a really long time




# #categorical variables - obtain factor levels
# COMMUNITY1<-terra::vect("../enviro datasets/NESP predicted distribution of seagrass communities/Shapefiles/GBR_NESP-TWQ-5.4_JCU_Seagrass-communities_20201203.shp")
# levels<-levels(as.factor(COMMUNITY1$COMMUNITY))
# # Recover the original factor levels for the aggregated raster
# preddat2022$COMMUNITY<-factor(preddat2022$COMMUNITY, levels=1:length(levels), labels=levels)
# preddat$COMMUNITY<-factor(preddat$COMMUNITY, levels=1:length(levels), labels=levels)

# builtfeature<-terra::vect("../enviro datasets/Breakwaters, groynes, seawalls, marinas/QSC_Extracted_Data_20231018_085138072000-40848/data.gdb")
# levels<-levels(as.factor(builtfeature$FEATURETYPE))
# preddat2022$builtfeature<- factor(preddat2022$builtfeature, levels=1:length(levels), labels=levels)
# preddat$builtfeature<- factor(preddat$builtfeature, levels=1:length(levels), labels=levels)

# geohab<-terra::vect("../enviro datasets/qld_geohab_av/czm/pristine_ests/release/qld/shape/geohab_qld_v2.shp")
# levels<-levels(as.factor(geohab$GH_TYPE))
# preddat2022$geohab<-factor(preddat2022$geohab, levels=1:length(levels), labels=levels)
# preddat$geohab<-factor(preddat$geohab, levels=1:length(levels), labels=levels)
# 
# geomorph<-terra::vect("../enviro datasets/Heap2008Geomorphology.shp")
# levels<-levels(as.factor(geomorph$feature))
# preddat2022$geomorph<-factor(preddat2022$geomorph, levels=1:length(levels), labels=levels)
# preddat$geomorph<-factor(preddat$geomorph, levels=1:length(levels), labels=levels)

write.csv(preddat, "preddat.grid.csv")
write.csv(preddat2022, "preddat2022.grid.csv")
```

```{r}
# preddat<-read.csv("preddat.grid.background.csv")
# preddat2022<-read.csv("preddat2022.grid.background.csv")
```
```{r}
raster_extent <- extent(mask)

# Extract XLLCORNER and YLLCORNER
xllcorner <- xmin(raster_extent)
yllcorner <- ymin(raster_extent)
```


```{r}
source("../Elith 2008 tutorial/brt.functions.R")
```
### crw
```{r}
dist2coast<-dist2coast1_2
preddat$dist2coast<-preddat$dist2coast1_2
preddat2022$dist2coast<-preddat2022$dist2coast1_2

unmodgrid2010crw<-gbm.predict.grids(final_modelunmod, # change this when you have all the rasters, this step takes a really long time
                  preddat, 
                  want.grids = T, 
                  sp.name ="crwunmod_preds",
                  pred.vec = rep(-9999,20428380), # adjust number of rows in your data
                  filepath = "brtpred/", 
                  num.col =4340, 
                  num.row = 4707, 
                  xll = xllcorner, 
                  yll = yllcorner, 
                  cell.size = 500, 
                  no.data = -9999,
                  plot=T)
saveRDS(unmodgrid2010crw, "grid2010unmodcrw.Rds")

# modgrid2010crw<-gbm.predict.grids(final_modelGL, 
#                   preddat, 
#                   want.grids = T, 
#                   sp.name ="crwGL_preds",
#                   pred.vec = rep(-9999,20428380), # adjust number of rows in your data
#                   filepath = "brtpred/", 
#                   num.col =4340, 
#                   num.row = 4707, 
#                   xll = xllcorner, 
#                   yll = yllcorner, 
#                   cell.size = 500, 
#                   no.data = -9999,
#                   plot=T)
# saveRDS(modgrid2010crw, "grid2010modcrw.Rds")
```
```{r}
unmodgrid2022crw<-gbm.predict.grids(final_modelunmod, # change this when you have all the rasters, this step takes a really long time
                  preddat2022, 
                  want.grids = T, 
                  sp.name ="crwunmod_preds2022",
                  pred.vec = rep(-9999,20428380), # adjust number of rows in your data
                  filepath = "brtpred/", 
                  num.col =4340, 
                  num.row = 4707, 
                  xll = xllcorner, 
                  yll = yllcorner, 
                  cell.size = 500, 
                  no.data = -9999,
                  plot=T)
saveRDS(unmodgrid2022crw, "grid2022unmodcrw.Rds")
# 
# modgrid2022crw<-gbm.predict.grids(BRT.crwGL, 
#                   preddat2022, 
#                   want.grids = T, 
#                   sp.name ="crwGL_preds2022",
#                   pred.vec = rep(-9999,20428380), # adjust number of rows in your data
#                   filepath = "brtpred/", 
#                   num.col =4340, 
#                   num.row = 4707, 
#                   xll = xllcorner, 
#                   yll = yllcorner, 
#                   cell.size = 500, 
#                   no.data = -9999,
#                   plot=T)
# saveRDS(modgrid2022crw, "grid2022modcrw.Rds")
```

```{r}

# unmodgrid2022crw<-readRDS("grid2022unmodcrw.Rds")
# 
# unmodgrid2010crw<-readRDS("grid2010unmodcrw.Rds")
# 
# modgrid2022crw<-readRDS("grid2022modcrw.Rds")
# modgrid2010crw<-readRDS("grid2010modcrw.Rds")

# how much area in 2010 vs 2022
sum(unmodgrid2022crw>0.5)*250000/1000000
sum(unmodgrid2010crw>0.5)*250000/1000000
# sum(modgrid2022crw>0.5)*250000/1000000
# sum(modgrid2010crw>0.5)*250000/1000000
```
```{r}
#% within 50km of coastline
18290/sum(unmodgrid2010crw>0.5)
15985/sum(unmodgrid2022crw>0.5)
```
This was calculated in QGIS by 
1. extracting raster by mask where mask layer was a 50km buffer around coastline
2. retaining only cells with a value >0.5
3. using zonal statistics to calculate the sum of remaining cells.

# Difference evaluation
Area? 
```{r}
sum(backgroundGL_preds>0.5) # number of 500m pixels with more than 50% probability of presence
sum(backgroundGL_preds2022>0.5)/893690 # way less in 2022
sum(backgroundunmod_preds>0.5) 
sum(backgroundunmod_preds2022>0.5)/893690 # way less in 2022

sum(crwGL_preds2022>0.5)/893690 # number of 500m pixels with more than 50% probability of presence
sum(crwunmod_preds2022>0.5)/893690 # way less in 2022, 34608
sum(crwunmod_preds>0.5)/893690
sum(crwunmod_preds2022>0.5)/893690 # way less in 2022
```

```{r}
sum(preddat2022$deamangrove<5000, na.rm=T)
sum(preddat$deamangrove<5000, na.rm=T) # about the same

sum(preddat$ZooL_N<3, na.rm=T)
sum(preddat2022$ZooL_N<3, na.rm=T) # much more in 2022

sum(preddat$EFI<0.1, na.rm=T)
sum(preddat2022$EFI<0.1, na.rm=T) # a bit less in 2022, but not heaps
```

```{r}
0.25*0.02364019*93690
```


2010 vs 2022
```{r}
hist(preddat$ZooL_N)
hist(preddat2022$ZooL_N)

hist(preddat$salt)
hist(preddat2022$salt)

hist(preddat$Secchi)
hist(preddat2022$Secchi)
```



# % overlap with protected zones and ports
```{r}
zones<-st_read("../enviro datasets/Great_Barrier_Reef_Marine_Park_Zoning/Great_Barrier_Reef_Marine_Park_Zoning.shp")
ports<-st_read("../../enviro shapefiles/Port_ERMP/Ports_GBR.shp")

crwunmod2022<-raster("brtpred/crwunmod_preds2022.asc")
```
```{r}
# reproject
crs(crwunmod2022)<-"EPSG:3577"


zones <- st_transform(zones, crs(crwunmod2022))
ports<- st_transform(ports, crs(crwunmod2022))

```
```{r }
# counting pixels in polygons

library(exactextractr) #calculates the fraction of each pixel that lies within the polygon boundaries and uses this fraction in its calculations

# Perform the extraction
coverage_stats <- exact_extract(crwunmod2022, zones, fun = function(values, coverage_fraction) {
  sum(values>0.5&!is.na(values) * coverage_fraction)
}, progress = FALSE)

# `coverage_stats` will contain the count of pixels for each polygon. You might want to add this back to your polygon data
zones$pixel_count <- coverage_stats

# If you need to aggregate by an attribute, you can use dplyr as shown previously
zones <- st_as_sf(zones) # Ensure it's an sf object for dplyr operations
summary_df <- zones %>%
  group_by(TYPE) %>%
  summarise(total_pixels = sum(pixel_count, na.rm = TRUE))

pixels_greater_than_0_5 <- crwunmod2022 > 0.5

# Now, use getValues to retrieve the raster values of this logical raster
values <- getValues(pixels_greater_than_0_5)

# Since values are logical (TRUE/FALSE), summing them counts the TRUE values.
# TRUE values are coerced to 1, and FALSE values are coerced to 0.
# na.rm = TRUE is used here to remove NA values before the summation.
habitat <- sum(values, na.rm = TRUE)

summary_df$percentage_area<-round(summary_df$total_pixels/habitat,3)
print(asummary_df)


### ports
coverage_stats <- exact_extract(crwunmod2022, ports, fun = function(values, coverage_fraction) {
  sum(values>0.5&!is.na(values) * coverage_fraction)
}, progress = FALSE)

# `coverage_stats` will contain the count of pixels for each polygon. You might want to add this back to your polygon data
ports$pixel_count <- coverage_stats
ports <- st_as_sf(ports) # Ensure it's an sf object for dplyr operations
summary_dfports <- ports %>%
  summarise(total_pixels = sum(pixel_count, na.rm = TRUE))


summary_dfports$percentage_area<-round(summary_dfports$total_pixels/habitat,3)
```
How much fell outside the marine park boundary (i.e. higher than mid water)
```{r}
GBRMP<-st_read("../enviro datasets/Great Barrier Reef Marine Park Boundary.shp")
GBRMP <- st_transform(GBRMP, crs(crwunmod2022))
coverage_stats <- exact_extract(crwunmod2022, GBRMP, fun = function(values, coverage_fraction) {
  sum(values>0.5&!is.na(values) * coverage_fraction)
}, progress = FALSE)

# `coverage_stats` will contain the count of pixels for each polygon. You might want to add this back to your polygon data
GBRMP$pixel_count <- coverage_stats
GBRMP <- st_as_sf(GBRMP) # Ensure it's an sf object for dplyr operations
summary_dfGBRMP <- GBRMP %>%
  summarise(total_pixels = sum(pixel_count, na.rm = TRUE))


summary_dfGBRMP$percentage_area<-round(summary_dfGBRMP$total_pixels/habitat,3)
```

```{r}
# examining spatial overlay of layers
raster_df<-as.data.frame(crwunmod2022, xy=T)
ggplot() +
  geom_raster(data = raster_df, aes(x = x, y = y, fill = crwunmod_preds2022)) +
  geom_sf(data = zones, fill = NA, color = "blue") +
  geom_sf(data = GBRMP, fill = NA, color = "red") +
  geom_sf(data= ports, fill=NA, color="yellow")+
  scale_fill_viridis_c() +
  theme_minimal() +
  labs(title = "Raster and Polygon Overlay", fill = "Raster value") +
  theme(legend.position = "bottom")
```

