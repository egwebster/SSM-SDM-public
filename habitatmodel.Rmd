---
title: "Untitled"
output: html_document
date: "2024-01-16"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
source("PA-paper/LoadPackages_PA.R")
library(sf)
```


```{r}
track1<-read.csv("20240212_track.csv")
#remove spatial/temporal where there is too much missing predictor data
track1%>%arrange(date)%>%visdat::vis_miss()
#background%>%slice_sample(prop=0.2)%>%arrange(date)%>%visdat::vis_miss()
track_filtered<-track1%>%filter(!is.na(Secchi))%>%filter(!is.na(date))

crw%>%arrange(date)%>%visdat::vis_miss()
crw_filtered<-crw%>%filter(!is.na(Secchi))
background_filtered<-background%>%filter(!is.na(Secchi))%>%dplyr::select(!X.6)%>%filter(!is.na(date))
#
#
# data<-rbind(crw_filtered, track_filtered)%>%select(!c(X.4, X.3, X.2, X.1))%>%
#   mutate(COMMUNITY=factor(COMMUNITY),
#          builtfeature=factor(builtfeature),
#          geohab=factor(geohab),
#          recboating=factor(recboating),
#          marina=factor(marina),
#          geomorph=factor(geomorph),
#          dist2coast=dist2coast1_2)%>%
#   mutate(data_type=ifelse(data_type=="track", 1, 0))

common_cols<-intersect(colnames(track_filtered), colnames(nas))

data.background<-rbind(
  subset(background_filtered, select=common_cols),
  subset(track_filtered, select=common_cols),
  subset(nas, select=common_cols))%>%
    dplyr::select(!c(X.3, X.2, X.1, X))%>%
  mutate(COMMUNITY=factor(COMMUNITY),
         builtfeature=factor(builtfeature),
         geohab=factor(geohab),
         recboating=factor(recboating),
         marina=factor(marina),
         geomorph=factor(geomorph),
         dist2coast=dist2coast1_2)%>%
  mutate(data_type=ifelse(data_type=="track", 1, 0))
```




```{r}
# separate GL and unmodified turtles
# library(readxl)
# metadata<-read.csv("tracked.turtls.csv")
# glimpse(metadata)
# 
# crw_filtered$PTT<-as.numeric(gsub(".*_([0-9]+)_.+", "\\1", crw_filtered$id))
# # Animals who were incorrecly re-assigned ids based on PTT
# #_96777 is everything after 2014 currently named QA46109_96777
# #QA64318_133758 is everything after 2017 currently named QA33394_133758
# crw<-crw_filtered%>%mutate(id=ifelse(PTT=="96777" & as.Date(date)>as.Date("2014-01-01"), paste("", gsub("^.*?(_)", "\\1", id), sep=""), id),
#                 id=ifelse(PTT=="133758"& as.Date(date)>as.Date("2017-01-01"), paste("QA64318", gsub("^.*?(_)", "\\1", id), sep=""), id))
# crw$Primary.tag<-gsub("^([^_]*)_.*$", "\\1", crw$id)
# 
# 
# 
# metadata<-metadata%>%select(c(PTT, Primary.tag, Deploy.site, Year, Sex, Maturity, CCL))
# 
# crw<-crw%>%left_join(metadata, by=c("PTT", "Primary.tag"))
# crw<-crw%>%mutate(Sex=factor(Sex), 
#                               Maturity=factor(Maturity))


```

```{r}
# 
# 
# # animals incorrectly assigned based on PTT
# data.background$PTT<-as.numeric(gsub(".*_([0-9]+)_.+", "\\1", data.background$id))
# data.background<-data.background%>%mutate(id=ifelse(PTT=="96777" & as.Date(date.x)>as.Date("2014-01-01"), paste("", gsub("^.*?(_)", "\\1", id), sep=""), id),
#                 id=ifelse(PTT=="133758"& as.Date(date.x)>as.Date("2017-01-01"), paste("QA64318", gsub("^.*?(_)", "\\1", id), sep=""), id))
# 
# 
# data.background$Primary.tag<-gsub("^([^_]*)_.*$", "\\1", data.background$id)
# #data.background$date<-as.POSIXct(data.background$date, format="%Y-%m-%d %H:%M:%S")
# 
# 
# 
# metadata<-metadata%>%dplyr::select(c(PTT, Primary.tag, Deploy.site, Year, Sex, Maturity, CCL))
# 
# data_join_b<-data.background%>%left_join(metadata, by=c("PTT", "Primary.tag"))
# data_join_b<-data_join_b%>%mutate(Sex=factor(Sex), 
#                               Maturity=factor(Maturity))%>%ungroup()

#write.csv(data_join_b, file="20240219data.background.csv")
# appended slope and ruggedness here, and clipped to seagrass mask
data_join_b<-read.csv("enviro datasets/20240222backgrounddata_clipped.csv")
# track<-data_join_b%>%filter(data_type=="true")

data_join_b$data_type<-ifelse(data_join_b$data_type=="false", 0, 1)
data_join_b<-data_join_b%>%mutate(COMMUNITY=factor(COMMUNITY), 
                                    builtfeature=factor(builtfeature),
                                    geohab=factor(geohab),
                                    geomorph=factor(geomorph))
data.background<-data_join_b
GL_b<-data_join_b%>%filter(Deploy.site=="GL")
unmodified_b<-data_join_b%>%filter(!Deploy.site%in%c("GL"))

# crw<-crw%>%select(!c(field_1, X.3, X.2, X.1, X))%>%
#   mutate(data_type="false")%>%
#   rename(dist2coast=dist2coast21)
# common_cols<-intersect(colnames(track), colnames(crw))
# 
# data_join<-rbind(
#   subset(crw, select=common_cols),
#   subset(track, select=common_cols))%>%
#   mutate(COMMUNITY=factor(COMMUNITY),
#          builtfeature=factor(builtfeature),
#          geohab=factor(geohab),
#          recboating=factor(recboating),
#          marina=factor(marina),
#          geomorph=factor(geomorph))%>%
#   mutate(data_type=ifelse(data_type=="true", 1, 0))
# write.csv(data_join, "20240304data.crw.csv")
# last?! run from hpc:
# crw<-read.csv("hydro_3dimscrw.csv")%>%select(!X.1)
# 
# #crw.sub<-crw%>%select(c(id, lon, lat, data_type, iteration, mean_wspeed, date, aus.date))
# 
# data_join<-read.csv("20240304data.crw.csv")%>%filter(data_type==1)
# 
# 
# data_join<-rbind(data_join, crw)
# write.csv(data_join, "20240309data.crw.csv")
data_join<-read.csv("20240309data.crw.csv")

data_join<-data_join%>%mutate(COMMUNITY=factor(COMMUNITY), 
                                    builtfeature=factor(builtfeature),
                                    geohab=factor(geohab),
                                    geomorph=factor(geomorph))%>%
  select(!X)

GL<-data_join%>%filter(Deploy.site=="GL")
unmodified<-data_join%>%filter(!Deploy.site%in%c("GL"))
```



```{r}
#sample data (75% training 25% testing)
# library(caret)
set.seed(123)
splitIndex <- createDataPartition(data_join$data_type, p = 0.75, list = FALSE)
train_data <- data_join[splitIndex, ]
test_data <- data_join[-splitIndex, ]

# GL sample
splitIndexGL <- createDataPartition(GL$data_type, p = 0.75, list = FALSE)
train_dataGL <- GL[splitIndexGL, ]
test_dataGL <- GL[-splitIndexGL, ]
# unmodified sample
splitIndexunmodified <- createDataPartition(unmodified$data_type, p = 0.75, list = FALSE)
train_dataunmodified <- unmodified[splitIndexunmodified, ]
test_dataunmodified <- unmodified[-splitIndexunmodified, ]
```
```{r}
set.seed(123)
splitIndex_b <- createDataPartition(data_join_b$data_type, p = 0.75, list = FALSE)
train_data_b <- data_join_b[splitIndex_b, ]
test_data_b <- data_join_b[-splitIndex_b, ]

# GL sample
splitIndexGL_b <- createDataPartition(GL_b$data_type, p = 0.75, list = FALSE)
train_dataGL_b <- GL_b[splitIndexGL_b, ]
test_dataGL_b <- GL_b[-splitIndexGL_b, ]
# unmodified sample
splitIndexunmodified_b <- createDataPartition(unmodified_b$data_type, p = 0.75, list = FALSE)
train_dataunmodified_b <- unmodified_b[splitIndexunmodified_b, ]
test_dataunmodified_b <- unmodified_b[-splitIndexunmodified_b, ]
```

correlation matrix
```{r}
data.num<-select_if(data_join, is.numeric)# include only numeric
data4cor<-data.num[,c(5:7,9:24, 28:29)]
corr<-cor(data4cor, method='pearson', use='pairwise.complete.obs')
round(corr, 3)
# extract pairs with 0.7 coeficient or higher
w <- which(abs(corr)>=0.7 & row(corr)<col(corr), arr.ind=TRUE)
## reconstruct names from positions
high_cor <- matrix(colnames(corr)[w],ncol=2)
high_cor
```
dist2recboats, dist2rivers, deamangrove
bathy, mmpexp, mmpfreq, Secchi, deamangrove, dist2coast
u, v
ruggedness,slope
```{r}
data.num<-select_if(data_join_b, is.numeric)# include only numeric
data4cor<-data.num[,c(5:7,9:29)]
corr<-cor(data4cor, method='pearson', use='pairwise.complete.obs')
round(corr, 3)
# extract pairs with 0.7 coeficient or higher
w <- which(abs(corr)>=0.7 & row(corr)<col(corr), arr.ind=TRUE)
## reconstruct names from positions
high_cor <- matrix(colnames(corr)[w],ncol=2)
high_cor
```
With background data, dist2recboat, dist2rivers, dist2coast, deamangrove all correlated
and mmpexposure and mmpfrequency
and meanwspeed and wspeed_u

```{r}
# plotting relationship of each predictor with response to inform var.monotone
data_join%>%arrange(ruggedness1)%>%
  mutate(group=ceiling(row_number()/100))%>%
  group_by(group)%>%
  summarise(mean_binom=mean(data_type), 
            mean_var=mean(ruggedness1))%>%
  ggplot()+
  geom_point(aes(y=mean_binom, x=mean_var))
```
dist2recboatfeat- 0
dist2rivers- 0
bathy- -1
dist2coast- -1
dist2reefs- -1
seagrass- 0
tidalexposure-0 
mmpexposure-0
mmpfrequency 1
salt 0
temp 0
mean_cur 0
u 0
v 0
mean_wspped 0
wspeed_u -1
wspeed_v -1
Zool_N 0
EFI 0
Secchi 0
deamangroves -1
slope -1
ruggedness -1

```{r}
# plotting relationship of each predictor with response to inform var.monotone
data_join_b%>%arrange(slope1)%>%
  mutate(group=ceiling(row_number()/100))%>%
  group_by(group)%>%
  summarise(mean_binom=mean(dist2recboatfeat), 
            mean_var=mean(slope1))%>%
  ggplot()+
  geom_point(aes(y=mean_binom, x=mean_var))
```

dist2recboatfeat- -1
dist2rivers- -1
bathy- -1
dist2coast- -1
dist2reefs- -1
seagrass- 0
tidalexposure-0 
mmpexposure-0
mmpfrequency 0
salt 0
temp 0
mean_cur 0
u 0
v 0
mean_wspped -1
wspeed_u -1
wspeed_v -1
Zool_N 0
EFI 0
Secchi 0
deamangroves -1
slope -1
ruggedness -1


```{r}
#model runs

#brt, gradient boosted models

try(BRT.crw<-gbm.fixed(data=train_data, 
                       gbm.x=c(6:8, 10, 12:13,15:19, 22:24,27:33,41:42), 
                       gbm.y=4, 
                       family="bernoulli",
                       var.monotone=c(0,0,0,0,0,-1,-1,0,0,0,1,0,0,0,0,0,0,0,0,-1,-1,-1,-1), # dist 2 coast has max values at coast
                       tree.complexity=5, 
                       learning.rate = 0.005, 
                       n.trees = 9000, 
                       bag.fraction=0.75)) 


#brtgl?
train_dataGL<-train_dataGL%>%select(!X.1)
try(BRT.crwGL<-gbm.fixed(data=train_dataGL, 
                       gbm.x=c(6:8, 10, 12:13,15:19, 22:24,27:33,41:42), 
                       gbm.y=4, 
                       family="bernoulli",
                       var.monotone=c(0,0,0,0,0,-1,-1,0,0,0,1,0,0,0,0,0,0,0,0,-1,-1,-1,-1),
                       tree.complexity=5, 
                       learning.rate = 0.005, 
                       n.trees = 9000, 
                       bag.fraction=0.75)) 
#brtunmodified
try(BRT.crwunmod<-gbm.fixed(data=train_dataunmodified, 
                       gbm.x=c(6:8, 10, 12:13,15:19, 22:24,27:33,41:42), 
                       gbm.y=4, 
                       family="bernoulli",
                       var.monotone=c(0,0,0,0,0,-1,-1,0,0,0,1,0,0,0,0,0,0,0,0,-1,-1,-1,-1),
                       tree.complexity=5, 
                       learning.rate = 0.005, 
                       n.trees = 9000, 
                       bag.fraction=0.75))
saveRDS(BRT.crw, "BRT.crw.Rds")
saveRDS(BRT.crwGL, "BRT.crwGL1.Rds")
saveRDS(BRT.crwunmod, "BRT.crwunmod.Rds")

```
```{r}


# models with background sampling
try(BRT.background1<-gbm.fixed(data=as.data.frame(train_data_b), 
                       gbm.x=c(8:10, 12,14:15, 17:21,24:26, 29,32:37,46:47), 
                       gbm.y=6, 
                       family="bernoulli",
                       var.monotone=
                         c(0,0,0,-1,-1,-1,-1,0,0,0,0,0,0,0,-1,0,0,0,0,-1,-1, -1,-1), 
                       tree.complexity=5, 
                       learning.rate = 0.005, 
                       n.trees = 2000, 
                       bag.fraction=0.75)) 


#brtgl?
try(BRT.backgroundGL<-gbm.fixed(data=as.data.frame(train_dataGL_b), 
                       gbm.x=c(8:10, 12,14:15, 17:21,24:26, 29,32:37,46:47), 
                       gbm.y=6, 
                       family="bernoulli",
                       var.monotone=
                         c(0,0,0,-1,-1,-1,-1,0,0,0,0,0,0,0,-1,0,0,0,0,-1,-1, -1,-1), 
                       tree.complexity=5, 
                       learning.rate = 0.005, 
                       n.trees = 2000, 
                       bag.fraction=0.75)) 
#brtunmodified
try(BRT.backgroundunmod<-gbm.fixed(data=as.data.frame(train_dataunmodified_b), 
                       gbm.x=c(8:10, 12,14:15, 17:21,24:26, 29,32:37,46:47), 
                       gbm.y=6,  
                       family="bernoulli",
                       var.monotone=
                         c(0,0,0,-1,-1,-1,-1,0,0,0,0,0,0,0,-1,0,0,0,0,-1,-1, -1,-1), 
                       tree.complexity=5, 
                       learning.rate = 0.005, 
                       n.trees = 2000, 
                       bag.fraction=0.75))
saveRDS(BRT.background1, "BRT.background1.Rds")
saveRDS(BRT.backgroundGL, "BRT.backgroundGL.Rds")
saveRDS(BRT.backgroundunmod, "BRT.backgroundunmod.Rds")
```




## load pre-run mods 25/01/2024
```{r}
BRT.crw<-readRDS("BRT.crw.Rds")
BRT.crwGL<-readRDS("BRT.crwGL.Rds")
BRT.crwunmod<-readRDS("BRT.crwunmod.Rds")
#test<-readRDS("BRT.crwGL1.Rds")

BRT.background1<-readRDS("BRT.background1.Rds")
BRT.backgroundGL<-readRDS("BRT.backgroundGL.Rds")
BRT.backgroundunmod<-readRDS("BRT.backgroundunmod.Rds")
```



Must first check, pruning.
```{r}
# perform out of bag method
(best.iterGL=gbm.perf(BRT.crwGL, method="OOB"))
(best.iterunmod=gbm.perf(BRT.crwunmod, method="OOB"))
(best.iter=gbm.perf(BRT.crw, method="OOB"))

(best.iterGLb=gbm.perf(BRT.backgroundGL, method="OOB"))
(best.iterunmodb=gbm.perf(BRT.backgroundunmod, method="OOB"))
(best.iterb=gbm.perf(BRT.background1, method="OOB"))
```
Shows mean change in predictive deviance. Blue dotted line shows number of trees at which this reaches a minimum. 
```{r}
#variable importance
summary(BRT.crw, n.trees=best.iter) # temperature, large zooplankton, distance to rec boat facilities, dist2rivers, salinity

# anything with >3.3 they're explaining something, and the others aren't explaining very much
```
```{r}
# get a list of predictors in alphabetical order
a<-summary(BRT.background1, n.trees=best.iterb)$var
sorted<-sort(a)
sorted
```

```{r}
# GL vs unmod
summary(BRT.crwGL, n.trees=best.iterGL) #Zoo, temp, salt, rec boats
summary(BRT.crwunmod, n.trees=best.iterunmod) #dist2 coast, seagrass, dist2 rivers, rec boats (not Zoo)
```
```{r}
b<-summary(BRT.background1, n.trees=best.iterb)
GLb<-summary(BRT.backgroundGL, n.trees=best.iterGLb)
unmodb<-summary(BRT.backgroundunmod, n.trees=best.iterunmodb)
c<-summary(BRT.crw, n.trees=best.iter)%>%
  mutate(var=ifelse(var=="dist2coast", "dist2coast1_2", var))
GLc<-summary(BRT.crwGL, n.trees=best.iterGL)%>%
  mutate(var=ifelse(var=="dist2coast", "dist2coast1_2", var))
unmodc<-summary(BRT.crwunmod, n.trees=best.iterunmod)%>%
  mutate(var=ifelse(var=="dist2coast", "dist2coast1_2", var))

combined_df<-reduce(list(b, GLb, unmodb, c, GLc, unmodc), full_join, by="var")
b.df<-combined_df%>%
  rename(background_all=rel.inf.x,
         background_modified=rel.inf.y,
         background_unmodified=rel.inf.x.x, 
         crw_all=rel.inf.y.y, 
         crw_modified=rel.inf.x.x.x, 
         crw_unmodified=rel.inf.y.y.y)%>%arrange(tolower(var))
b.df

# rename stuff for plotting
names(b.df)[names(b.df)=="var"]<-"Variable"
b.df$"Variable"<-c("Bathymetry", "Built features", "Seagrass community type", "Distance to mangroves", "Distance to coast", "Distance to boat ramps", "Distance to reefs", "Distance to rivers", "Ecology fine inorganics", "Geohabitat", "Geomorphology", "Mean seawater velocity", "Mean wind speed", "Chronic exposure to floodwater", "Acute flood frequency", "Ruggedness", "Salinity", "Seagrass probability", "Secchi depth", "Slope", "Temperature", "Tidal exposure", "Large zooplankton nitrogen")
```



```{r}
#source("PA-paper/ModelFunctions.R")
#source("PA-paper/Model_Eval_Fcns.R")
pred=predict.gbm(BRT.crw,newdata=test_data,type="response", n.trees=BRT.crw$gbm.call$best.trees,na.rm=F)

predGL=predict.gbm(BRT.crwGL,newdata=test_dataGL,type="response", n.trees=BRT.crwGL$gbm.call$best.trees,na.rm=F)

predunmod=predict.gbm(BRT.crwunmod,newdata=test_dataunmodified,type="response", n.trees=BRT.crwunmod$gbm.call$best.trees,na.rm=F)
```
https://besjournals.onlinelibrary.wiley.com/doi/10.1111/j.1365-2656.2008.01390.x#b18 
https://rspatial.org/raster/sdm/6_sdm_methods.html
```{r}
#AUC and TSS (true skill statistic)- predictive skill

Evaluations_7525 <- as.data.frame(matrix(data=0,nrow=1,ncol=3))
  colnames(Evaluations_7525) <- c("AUC","TSS","TPR")
#dev <- calc.deviance(obs=test_data$data_type, pred=pred, calc.mean=TRUE)
#roc<-roc(test_data$data_type, pred)
  d <- cbind(test_data$data_type, pred)
  pres <- d[d[,1]==1,2]
  abs <- d[d[,1]==0,2]
  e <- evaluate(p=pres, a=abs)
  Evaluations_7525[1,1] <- e@auc
  Evaluations_7525[1,2] <- max(e@TPR + e@TNR-1)
  Evaluations_7525[1,3] <- mean(e@TPR)
  
(Evaluations_7525)
eval<-c(names(Evaluations_7525), "% deviance explained")
b.df[24:27,1]<-eval
b.df[24,5]<-Evaluations_7525[,1]
b.df[25,5]<-Evaluations_7525[,2]
b.df[26,5]<-Evaluations_7525[,3]
```
AUC of 0.5 means the model is just as good as a random guess. AUC varies with spatial extent used to select background points. Larger extent, higher AUC. 

# Deviance explained - explanatory power
```{r}
x<-(BRT.crw$self.statistics$null.deviance-BRT.crw$self.statistics$resid.deviance)/BRT.crw$self.statistics$null.deviance
b.df[27,5]<-x
```
GL and unmod diagnostics
```{r}
#AUC and TSS (true skill statistic)- predictive skill

Evaluations_7525GL <- as.data.frame(matrix(data=0,nrow=1,ncol=3))
  colnames(Evaluations_7525GL) <- c("AUC","TSS","TPR")
#dev <- calc.deviance(obs=test_data$data_type, pred=pred, calc.mean=TRUE)
#roc<-roc(test_data$data_type, pred)
  d <- cbind(test_dataGL$data_type, predGL)
  pres <- d[d[,1]==1,2]
  abs <- d[d[,1]==0,2]
  e <- evaluate(p=pres, a=abs)
  Evaluations_7525GL[1,1] <- e@auc
  Evaluations_7525GL[1,2] <- max(e@TPR + e@TNR-1)
  Evaluations_7525GL[1,3] <- mean(e@TPR)
  
(Evaluations_7525GL)
b.df[24,6]<-Evaluations_7525GL[,1]
b.df[25,6]<-Evaluations_7525GL[,2]
b.df[26,6]<-Evaluations_7525GL[,3]
  
Evaluations_7525unmod <- as.data.frame(matrix(data=0,nrow=1,ncol=3))
  colnames(Evaluations_7525unmod) <- c("AUC","TSS","TPR")
#dev <- calc.deviance(obs=test_data$data_type, pred=pred, calc.mean=TRUE)
#roc<-roc(test_data$data_type, pred)
  d <- cbind(test_dataunmodified$data_type, predunmod)
  pres <- d[d[,1]==1,2]
  abs <- d[d[,1]==0,2]
  e <- evaluate(p=pres, a=abs)
  Evaluations_7525unmod[1,1] <- e@auc
  Evaluations_7525unmod[1,2] <- max(e@TPR + e@TNR-1)
  Evaluations_7525unmod[1,3] <- mean(e@TPR)
  
(Evaluations_7525unmod)
b.df[24,7]<-Evaluations_7525unmod[,1]
b.df[25,7]<-Evaluations_7525unmod[,2]
b.df[26,7]<-Evaluations_7525unmod[,3]
```
```{r}
#GL
y<-(BRT.crwGL$self.statistics$null.deviance-BRT.crwGL$self.statistics$resid.deviance)/BRT.crwGL$self.statistics$null.deviance
b.df[27,6]<-y

#unmod
z<-(BRT.crwunmod$self.statistics$null.deviance-BRT.crwunmod$self.statistics$resid.deviance)/BRT.crwunmod$self.statistics$null.deviance
b.df[27,7]<-z
```


#####background
```{r}
predb=predict.gbm(BRT.background1,newdata=test_data_b,type="response", n.trees=BRT.background1$gbm.call$best.trees,na.rm=F)

predGLb=predict.gbm(BRT.backgroundGL,newdata=test_dataGL_b,type="response", n.trees=BRT.backgroundGL$gbm.call$best.trees,na.rm=F)

predunmodb=predict.gbm(BRT.backgroundunmod,newdata=test_dataunmodified_b,type="response", n.trees=BRT.backgroundunmod$gbm.call$best.trees,na.rm=F)
```
https://besjournals.onlinelibrary.wiley.com/doi/10.1111/j.1365-2656.2008.01390.x#b18 
https://rspatial.org/raster/sdm/6_sdm_methods.html
```{r}
#AUC and TSS (true skill statistic)- predictive skill

Evaluations_7525 <- as.data.frame(matrix(data=0,nrow=1,ncol=3))
  colnames(Evaluations_7525) <- c("AUC","TSS","TPR")
#dev <- calc.deviance(obs=test_data$data_type, pred=pred, calc.mean=TRUE)
#roc<-roc(test_data$data_type, pred)
  d <- cbind(test_data_b$data_type, predb)
  pres <- d[d[,1]==1,2]
  abs <- d[d[,1]==0,2]
  e <- evaluate(p=pres, a=abs)
  Evaluations_7525[1,1] <- e@auc
  Evaluations_7525[1,2] <- max(e@TPR + e@TNR-1)
  Evaluations_7525[1,3] <- mean(e@TPR)
  
(Evaluations_7525)
transposed<-t(Evaluations_7525)
#eval<-c(names(Evaluations_7525), "% deviance explained")
b.df[24:27,1]<-eval
b.df[24,2]<-Evaluations_7525[,1]
b.df[25,2]<-Evaluations_7525[,2]
b.df[26,2]<-Evaluations_7525[,3]
```
AUC of 0.5 means the model is just as good as a random guess. AUC varies with spatial extent used to select background points. Larger extent, higher AUC. 
TSS= true skill statistic
TPR= true presence rate, correctly predicted presence observations over total presences (aka sensitivity or hit rate)

# Deviance explained - explanatory power
```{r}
a<-(BRT.background1$self.statistics$null.deviance-BRT.background1$self.statistics$resid.deviance)/BRT.background1$self.statistics$null.deviance
b.df[27,2]<-a
```
GL and unmod diagnostics
```{r}
#AUC and TSS (true skill statistic)- predictive skill

Evaluations_7525GL <- as.data.frame(matrix(data=0,nrow=1,ncol=3))
  colnames(Evaluations_7525GL) <- c("AUC","TSS","TPR")
#dev <- calc.deviance(obs=test_data$data_type, pred=pred, calc.mean=TRUE)
#roc<-roc(test_data$data_type, pred)
  d <- cbind(test_dataGL_b$data_type, predGLb)
  pres <- d[d[,1]==1,2]
  abs <- d[d[,1]==0,2]
  e <- evaluate(p=pres, a=abs)
  Evaluations_7525GL[1,1] <- e@auc
  Evaluations_7525GL[1,2] <- max(e@TPR + e@TNR-1)
  Evaluations_7525GL[1,3] <- mean(e@TPR)
  
(Evaluations_7525GL)
b.df[24,3]<-Evaluations_7525GL[,1]
b.df[25,3]<-Evaluations_7525GL[,2]
b.df[26,3]<-Evaluations_7525GL[,3]
```
```{r}
Evaluations_7525unmod <- as.data.frame(matrix(data=0,nrow=1,ncol=3))
  colnames(Evaluations_7525unmod) <- c("AUC","TSS","TPR")
#dev <- calc.deviance(obs=test_data$data_type, pred=pred, calc.mean=TRUE)
#roc<-roc(test_data$data_type, pred)
  d <- cbind(test_dataunmodified_b$data_type, predunmodb)
  pres <- d[d[,1]==1,2]
  abs <- d[d[,1]==0,2]
  e <- evaluate(p=pres, a=abs)
  Evaluations_7525unmod[1,1] <- e@auc
  Evaluations_7525unmod[1,2] <- max(e@TPR + e@TNR-1)
  Evaluations_7525unmod[1,3] <- mean(e@TPR)
  
(Evaluations_7525unmod)
b.df[24,4]<-Evaluations_7525unmod[,1]
b.df[25,4]<-Evaluations_7525unmod[,2]
b.df[26,4]<-Evaluations_7525unmod[,3]
```
```{r}
#GL
b<-(BRT.backgroundGL$self.statistics$null.deviance-BRT.backgroundGL$self.statistics$resid.deviance)/BRT.backgroundGL$self.statistics$null.deviance

#unmod
c<-(BRT.backgroundunmod$self.statistics$null.deviance-BRT.backgroundunmod$self.statistics$resid.deviance)/BRT.backgroundunmod$self.statistics$null.deviance
b.df[27,3]<-b
b.df[27,4]<-c
```


# Create a summary table w/heatmap for rel.inf
```{r}
library(gt)
library(webshot2)
threshold<-100/27

# Ensure columns are numeric and handle NAs
b.df[, c("background_all", "background_modified", "background_unmodified", "crw_all", "crw_modified", "crw_unmodified")] <- 
  lapply(b.df[, c("background_all", "background_modified", "background_unmodified", "crw_all", "crw_modified", "crw_unmodified")], 
         function(x) as.numeric(x))

gt_table <- b.df %>% 
  gt() %>%
  # Color the cells based on their values, excluding the last 4 rows
  data_color(
    columns = c("background_all", "background_modified", "background_unmodified", "crw_all", "crw_modified", "crw_unmodified"),
    rows = 1:(nrow(b.df)-4), # Exclude the last 4 rows
        fn = function(x) {
      # Use scales::col_numeric to create a color function
      scales::col_numeric(
        palette = c("blue", "yellow", "red"),
        domain = range(b.df[,2:4], na.rm = TRUE)
      )(x)
    }
  ) %>%
  fmt_number(
    columns = c("background_all", "background_modified", "background_unmodified", "crw_all", "crw_modified", "crw_unmodified"),
    decimals = 2 # Format numbers to two decimal places
  ) 
  
  gt_table <- gt_table %>%
  tab_style(
    style = cell_borders(
      sides = "all",
      color = "black",
      weight = px(3)
    ),
    locations = list(
        cells_body(
          columns = background_all, 
          rows = background_all>threshold
        ),
        cells_body(
          columns = background_modified, 
          rows = background_modified>threshold
        ), 
        cells_body(
          columns = background_unmodified, 
          rows = background_unmodified>threshold
        ), 
         cells_body(
          columns = crw_all, 
          rows = crw_all>threshold
        ), 
         cells_body(
          columns = crw_modified, 
          rows = crw_modified>threshold
        ), 
         cells_body(
          columns = crw_unmodified, 
          rows = crw_unmodified>threshold
        )
  )
)%>%tab_spanner_delim(
    delim = "_"
)
gt_table
gtsave(gt_table, filename="rel.importance.png")
```
# Legend
```{r}
legend_data <- data.frame(
  Value = seq(min(b.df[,2:4]), max(b.df[,2:4]), length.out = 100)
)

# Create the legend as a plot
legend_plot <- ggplot(legend_data, aes(x = Value, y = 1, fill = Value)) +
  geom_tile() +
  scale_fill_gradientn(colors = c("blue", "yellow", "red")) +
  theme_void() +
  theme(legend.position = "bottom", legend.title=element_blank(), legend.direction="vertical", legend.text=element_text(size=14)) 

# Plot the legend
library(cowplot)
legend<-get_legend(legend_plot)
p<-plot(legend)
ggsave(legend, filename="relimportance_legend.png", width=1, height=2)
```


# Explore partial effects (partial dependence plots)
i.e. what would the effect of the predictor be if you averaged out the effects of all the other predictors in the full model

yhat
- in logistic regression, is the log-odss of the probability of the outcome (presence) being 1.
- can transform to probability = 1/(1+exp(-yhat)) to give the probability of each observation being a presence.
- The y-axis of a partial dependence plot for regression represents the marginal impact of the independent variable to the dependent variable. E.g. if the line is at 0, then for that value of the independent variable, there is 0 impact to the dependent variable.


```{r}

nms <- colnames(train_data_b)
p <- vector('list', 6)
names(p) <- nms[c(19, 20, 7, 15, 11, 32)] #change these according to model rank list
for (nm in nms[c(19, 20, 7, 15, 11, 32)]) {
  print(nm)
  p[[nm]] <- BRT.background1 %>% pdp::partial(pred.var=nm,
                                 n.trees=best.iterb,
                                 train=train_data_b,
                                 inv.link=plogis,
                                 recursive=FALSE,
                                 type='regression') %>%
    autoplot() + ylim(0, 1)
}
 
do.call('grid.arrange', p)
```
```{r}
BRT.backgroundGL |> 
  pdp::partial(pred.var='mmpexposure1', #tell it what variable
               n.trees=best.iterb, #how many trees to cull after
               train=train_data_b,
               # inv.link=plogis,
               # recursive=F,
               type='regression'
               #prob=T
               #recursive=F, #fast approximation that only works for gaussian loss. If we didn't use gaussian, this can't be false.
               ) |> 
  autoplot()

nms <- colnames(train_data_b)
p <- vector('list', 5)
names(p) <- nms[c(8,12,33,20,37)] #change these according to model rank list
for (nm in nms[c(8,12,33,20,37)]) {
  print(nm)
  p[[nm]] <- BRT.backgroundGL %>% pdp::partial(pred.var=nm,
                                 n.trees=best.iterGLb,
                                 train=train_data_b,
                                 # inv.link=plogis,
                                 # recursive=FALSE,
                                 type='regression') %>%
    autoplot()+theme_classic()  #+ ylim(0, 1)
}
 
plot<-do.call('grid.arrange', p)
plot
# which categories best predict presence
pdp_df <- BRT.backgroundGL %>% pdp::partial(pred.var='COMMUNITY',
                                 n.trees=best.iterGLb,
                                 train=train_data_b,
                                 # inv.link=plogis,
                                 # recursive=FALSE,
                                 type='regression')

# Now, sort the data frame by the predicted probabilities (assuming 'y' column contains these)
sorted_pdp <- pdp_df[order(pdp_df$y, decreasing = TRUE), ]

# If you want to view the sorted categories alongside their probabilities
print(sorted_pdp)
```

```{r}
q <- vector('list', 4)
names(q) <- nms[c(8,36,21,32)] #change these according to model rank list
for (nm in nms[c(8,36,21,32)]) {
  print(nm)
  q[[nm]] <- BRT.backgroundunmod %>% pdp::partial(pred.var=nm,
                                 n.trees=best.iterunmodb,
                                 train=train_data_b,
                                 #inv.link=plogis,
                                 #recursive=FALSE,
                                 type='regression') %>%
    autoplot()+theme_classic()# + ylim(0, 1)
}
 
do.call('grid.arrange', q)
# which categories best predict presence
pdp_df <- BRT.backgroundunmod %>% pdp::partial(pred.var='COMMUNITY',
                                 n.trees=best.iterunmodb,
                                 train=train_data_b,
                                 # inv.link=plogis,
                                 # recursive=FALSE,
                                 type='regression')

# Now, sort the data frame by the predicted probabilities (assuming 'y' column contains these)
sorted_pdp <- pdp_df[order(pdp_df$y, decreasing = TRUE), ]

# If you want to view the sorted categories alongside their probabilities
print(sorted_pdp)
```
## crw
```{r}
nms <- colnames(train_data)
r <- vector('list', 8)
names(r) <- nms[c(11,13,29,30,42,14,7,42)] #change these according to model rank list
for (nm in nms[c(11,13,29,30,42,14,7,42)]) {
  print(nm)
  r[[nm]] <- BRT.crwGL %>% pdp::partial(pred.var=nm,
                                 n.trees=best.iterGL,
                                 train=train_data,
                                 # inv.link=plogis,
                                 # recursive=FALSE,
                                 type='regression') %>%
    autoplot()+theme_classic()  #+ ylim(0, 1)
}
 
plot<-do.call('grid.arrange', r)

# which categories best predict presence
pdp_df <- BRT.crwGL %>% pdp::partial(pred.var='COMMUNITY',
                                 n.trees=best.iterGL,
                                 train=train_data,
                                 # inv.link=plogis,
                                 # recursive=FALSE,
                                 type='regression')

# Now, sort the data frame by the predicted probabilities (assuming 'y' column contains these)
sorted_pdp <- pdp_df[order(pdp_df$y, decreasing = TRUE), ]

# If you want to view the sorted categories alongside their probabilities
print(sorted_pdp)
```

```{r}
s <- vector('list', 7)
names(s) <- nms[c(11,7,13,14,20,33,17)] #change these according to model rank list
for (nm in nms[c(11,7,13,14,20,33,17)]) {
  print(nm)
  s[[nm]] <- BRT.crwunmod %>% pdp::partial(pred.var=nm,
                                 n.trees=best.iterunmod,
                                 train=train_data,
                                 #inv.link=plogis,
                                 #recursive=FALSE,
                                 type='regression') %>%
    autoplot()+theme_classic()# + ylim(0, 1)
}
 
do.call('grid.arrange', s)
# which categories best predict presence
pdp_df <- BRT.crwunmod %>% pdp::partial(pred.var='COMMUNITY',
                                 n.trees=best.iterunmod,
                                 train=train_data,
                                 # inv.link=plogis,
                                 # recursive=FALSE,
                                 type='regression')

# Now, sort the data frame by the predicted probabilities (assuming 'y' column contains these)
sorted_pdp <- pdp_df[order(pdp_df$y, decreasing = TRUE), ]

# If you want to view the sorted categories alongside their probabilities
print(sorted_pdp)
```



Environmental distance
```{r}
bhattacharyya_distance <- function(mu1, sigma1, mu2, sigma2) {
  # Calculate the average variance
  sigma_avg <- (sigma1 + sigma2) / 2
  
  # Calculate the Bhattacharyya distance
  D_B <- 1/4 * log(1/4 * (sigma1/sigma2 + sigma2/sigma1 + 2)) + 1/4 * ((mu1 - mu2)^2 / sigma_avg)
  
  return(D_B)
}
```

```{r}
#library(pfc)

track_filtered<-data_join_b%>%filter(data_type=="1")
background_filtered<-data_join_b%>%filter(data_type=="0")
trackmmp<-track_filtered%>%filter(!is.na(mmpfrequency1))%>%select(mmpfrequency1)
backgroundmmp<-background_filtered%>%filter(!is.na(mmpfrequency1))%>%select(mmpfrequency1)
#disparity::bhatt.coeff(as.numeric(trackmmp), as.numeric(backgroundmmp)) no dice
spatialEco::separability(trackmmp, backgroundmmp, plot=T)

trackmmp<-track_filtered%>%filter(!is.na(mmpexposure1))%>%select(mmpexposure1)
backgroundmmp<-background_filtered%>%filter(!is.na(mmpexposure1))%>%select(mmpexposure1)
spatialEco::separability(trackmmp, backgroundmmp, na.rm=T, plot=T)

# MMP exposure
# using formula:
mu1<-mean(track_filtered$mmpexposure1, na.rm=T)
sigma1<-var(track_filtered$mmpexposure1, na.rm=T)
mu2<-mean(background_filtered$mmpexposure1, na.rm=T)
sigma2<-var(background_filtered$mmpexposure1, na.rm=T)
bhattacharyya_distance(mu1, sigma1, mu2, sigma2)

# coefficient
sqrt(2 * sigma1 * sigma2 / (sigma1^2 + sigma2^2)) * exp(-1/4 * ((mu1 - mu2)^2 / (sigma1^2 + sigma2^2)))


mu1<-mean(track_filtered$dist2recboatfeat, na.rm=T)
sigma1<-var(track_filtered$dist2recboatfeat, na.rm=T)
mu2<-mean(background_filtered$dist2recboatfeat, na.rm=T)
sigma2<-var(background_filtered$dist2recboatfeat, na.rm=T)
bhattacharyya_distance(mu1, sigma1, mu2, sigma2)

# coefficient
sqrt(2 * sigma1 * sigma2 / (sigma1^2 + sigma2^2)) * exp(-1/4 * ((mu1 - mu2)^2 / (sigma1^2 + sigma2^2)))
spatialEco::separability(track_filtered$dist2coast1_2, background_filtered$dist2coast1_2, plot=T)

mu1<-mean(track_filtered$ZooL_N, na.rm=T)
sigma1<-var(track_filtered$ZooL_N, na.rm=T)
mu2<-mean(background_filtered$ZooL_N, na.rm=T)
sigma2<-var(background_filtered$ZooL_N, na.rm=T)
bhattacharyya_distance(mu1, sigma1, mu2, sigma2)

# coefficient
sqrt(2 * sigma1 * sigma2 / (sigma1^2 + sigma2^2)) * exp(-1/4 * ((mu1 - mu2)^2 / (sigma1^2 + sigma2^2)))
trackzoo<-track_filtered%>%filter(!is.na(ZooL_N))%>%select(ZooL_N)
backgroundzoo<-background_filtered%>%filter(!is.na(ZooL_N))%>%select(ZooL_N)
spatialEco::separability(trackzoo, backgroundzoo, plot=T)


mu1<-mean(track_filtered$dist2coast, na.rm=T)
sigma1<-var(track_filtered$dist2coast, na.rm=T)
mu2<-mean(background_filtered$dist2coast1, na.rm=T)
sigma2<-var(background_filtered$dist2coast, na.rm=T)
# coefficient
sqrt(2 * sigma1 * sigma2 / (sigma1^2 + sigma2^2)) * exp(-1/4 * ((mu1 - mu2)^2 / (sigma1^2 + sigma2^2)))

mu1<-mean(track_filtered$mmpfrequency1, na.rm=T)
sigma1<-var(track_filtered$mmpfrequency1, na.rm=T)
mu2<-mean(background_filtered$mmpfrequency1, na.rm=T)
sigma2<-var(background_filtered$mmpfrequency1, na.rm=T)
# coefficient
sqrt(2 * sigma1 * sigma2 / (sigma1^2 + sigma2^2)) * exp(-1/4 * ((mu1 - mu2)^2 / (sigma1^2 + sigma2^2)))

mu1<-mean(track_filtered$deamangrove, na.rm=T)
sigma1<-var(track_filtered$deamangrove, na.rm=T)
mu2<-mean(background_filtered$deamangrove, na.rm=T)
sigma2<-var(background_filtered$deamangrove, na.rm=T)
# coefficient
sqrt(2 * sigma1 * sigma2 / (sigma1^2 + sigma2^2)) * exp(-1/4 * ((mu1 - mu2)^2 / (sigma1^2 + sigma2^2)))

#### categorical

levels <- union(track_filtered$COMMUNITY, background_filtered$COMMUNITY)
sample1_factor <- factor(track_filtered$COMMUNITY, levels = levels)
sample2_factor <- factor(background_filtered$COMMUNITY, levels = levels)

# Calculate probabilities for each category in both samples
prob1 <- table(sample1_factor) / length(sample1_factor)
prob2 <- table(sample2_factor) / length(sample2_factor)

# Calculate the Bhattacharyya coefficient
BC <- sum(sqrt(prob1 * prob2))
```
### crw
```{r}
track_filtered<-data_join%>%filter(data_type==1)
crw_filtered<-data_join%>%filter(data_type==0)
mu1<-mean(track_filtered$dist2recboatfeat, na.rm=T)
sigma1<-var(track_filtered$dist2recboatfeat, na.rm=T)
mu2<-mean(crw_filtered$dist2recboatfeat, na.rm=T)
sigma2<-var(crw_filtered$dist2recboatfeat, na.rm=T)
# coefficient
sqrt(2 * sigma1 * sigma2 / (sigma1^2 + sigma2^2)) * exp(-1/4 * ((mu1 - mu2)^2 / (sigma1^2 + sigma2^2)))


mu1<-mean(track_filtered$ZooL_N, na.rm=T)
sigma1<-var(track_filtered$ZooL_N, na.rm=T)
mu2<-mean(crw_filtered$ZooL_N, na.rm=T)
sigma2<-var(crw_filtered$ZooL_N, na.rm=T)
# coefficient
sqrt(2 * sigma1 * sigma2 / (sigma1^2 + sigma2^2)) * exp(-1/4 * ((mu1 - mu2)^2 / (sigma1^2 + sigma2^2)))


mu1<-mean(track_filtered$EFI, na.rm=T)
sigma1<-var(track_filtered$EFI, na.rm=T)
mu2<-mean(crw_filtered$EFI, na.rm=T)
sigma2<-var(crw_filtered$EFI, na.rm=T)
# coefficient
sqrt(2 * sigma1 * sigma2 / (sigma1^2 + sigma2^2)) * exp(-1/4 * ((mu1 - mu2)^2 / (sigma1^2 + sigma2^2)))

mu1<-mean(track_filtered$Dist2Rivers1, na.rm=T)
sigma1<-var(track_filtered$Dist2Rivers1, na.rm=T)
mu2<-mean(crw_filtered$Dist2Rivers1, na.rm=T)
sigma2<-var(crw_filtered$Dist2Rivers1, na.rm=T)
# coefficient
sqrt(2 * sigma1 * sigma2 / (sigma1^2 + sigma2^2)) * exp(-1/4 * ((mu1 - mu2)^2 / (sigma1^2 + sigma2^2)))

mu1<-mean(track_filtered$bathy1, na.rm=T)
sigma1<-var(track_filtered$bathy1, na.rm=T)
mu2<-mean(crw_filtered$bathy1, na.rm=T)
sigma2<-var(crw_filtered$bathy1, na.rm=T)
# coefficient
sqrt(2 * sigma1 * sigma2 / (sigma1^2 + sigma2^2)) * exp(-1/4 * ((mu1 - mu2)^2 / (sigma1^2 + sigma2^2)))


#### categorical

levels <- union(track_filtered$COMMUNITY, crw_filtered$COMMUNITY)
sample1_factor <- factor(track_filtered$COMMUNITY, levels = levels)
sample2_factor <- factor(crw_filtered$COMMUNITY, levels = levels)

# Calculate probabilities for each category in both samples
prob1 <- table(sample1_factor) / length(sample1_factor)
prob2 <- table(sample2_factor) / length(sample2_factor)

# Calculate the Bhattacharyya coefficient
BC <- sum(sqrt(prob1 * prob2))
```



# interactions?
int.size represents the number of times an interaction between the 2 variables is used in the decision trees (i.e. split was made considering both variables together)
```{r}
names(BRT.crw$gbm.call)[1]<-"dataframe"
find.int<-gbm.interactions(BRT.crw)
find.int
find.int$rank.list
```
```{r}
names(BRT.crwGL$gbm.call)[1]<-"dataframe"
find.int<-gbm.interactions(BRT.crwGL)
find.int
find.int$rank.list
```

```{r}
names(BRT.crwunmod$gbm.call)[1]<-"dataframe"
find.int<-gbm.interactions(BRT.crwunmod)
find.int
find.int$rank.list
```

```{r}
names(BRT.background1$gbm.call)[1]<-"dataframe"
find.intb<-gbm.interactions(BRT.background1)
find.intb
find.intb$rank.list
```
```{r}
names(BRT.backgroundGL$gbm.call)[1]<-"dataframe"
find.intb<-gbm.interactions(BRT.backgroundGL)
find.intb
find.intb$rank.list
```
```{r}
names(BRT.backgroundunmod$gbm.call)[1]<-"dataframe"
find.intb<-gbm.interactions(BRT.backgroundunmod)
find.intb
find.intb$rank.list
```

Plot pairwise interactions
```{r}
gbm.perspec(BRT.crw, 13,1)
gbm.perspec(BRT.crw, 23, 16)
gbm.perspec(BRT.crw, 15,5)
gbm.perspec(BRT.crw, 23,15)
gbm.perspec(BRT.crw, 23,12)
```
```{r}
gbm.perspec(BRT.background1, 13,1)
gbm.perspec(BRT.backgroundGL, 12,5)
gbm.perspec(BRT.backgroundunmod, 22, 8)
gbm.perspec(BRT.crw, 15,5)
gbm.perspec(BRT.crw, 23,15)
gbm.perspec(BRT.crw, 23,12)
```



# Predicting to grids


```{r}
# 2010 with tif
extract_tif_header_info_raster <- function(file_path) {
  r <- raster(file_path)
  ncols <- ncol(r)
  nrows <- nrow(r)
  cellsize <- res(r)[1] # Assuming uniform resolution in x and y
  
  # Return a list containing the extracted information
  return(list(ncols = ncols, nrows = nrows, cellsize = cellsize))
}


file_paths_tif <- list.files("enviro datasets/rasters for grid predictions/", pattern = "\\.tif$", full.names = TRUE)
header_info_tif <- lapply(file_paths_tif, extract_tif_header_info_raster)

#make mask the first item in the vector
file_paths_tif<-c(file_paths_tif[18], file_paths_tif[-c(18,19,36,4,12,14,2)])
# Identify indices of elements containing "2022"
indices_to_remove <- grep("2022", file_paths_tif)

# Exclude those elements from the vector
file_paths_tif <- file_paths_tif[-indices_to_remove]

# # e.g. Find the column number for "Age"
# column_number <- which(names(df) == "Age")
variable.names2010 <- c("mask", names(train_data_b)[c(15,9,8,37,12,17,14,33,10,35,36,26,29,20,21,46,24,18,34,47,25,19,32)]) #here make sure the order is the same as above, if you're using different data
processed_layers <- list() # Initialize an empty list to store processed layers

for(i in 1:length(file_paths_tif)) { 
  # Load each .tif file as a RasterLayer object
  r <- raster(file_paths_tif[i])
  cat(file_paths_tif[i], "\n")
  if (variable.names2010[i] %in% c("COMMUNITY", "geohab", "geomorph", "builtfeature")) {
    r_agg_raster<-r
  } else {
    

  # Convert RasterLayer to SpatRaster for more efficient processing
  r_spat <- terra::rast(r)
  
  # Aggregate the SpatRaster object
  r_agg <- aggregate(r_spat, fact=5, fun=mean, na.rm=TRUE)
  
  # Optionally: Convert back to RasterLayer if needed for compatibility with other raster package functions
  r_agg_raster <- raster(r_agg)
  }
  # Replace nodata value with NA - Note: Adjust according to your data's needs
  currentNodata <- NAvalue(r_agg_raster)
  cat("NoData Value: ", currentNodata, "\n") # all seem to be -Inf
  
  # Store the aggregated RasterLayer object in the list with the desired name
  processed_layers[[variable.names2010[i]]] <- r_agg_raster
  
  # Optionally, assign the RasterLayer object to the global environment with the desired name
  assign(variable.names2010[i], r_agg_raster, envir = .GlobalEnv)
}
```
```{r}
# rasters are too large for memory allocation. Need to aggregate to reduce resolution (to 500m)
```


```{r}
rasters_values <- lapply(variable.names2010, function(name) values(get(name)))
preddat <- data.frame(rasters_values) 
names(preddat) <- variable.names2010
preddat<- preddat[!is.na(preddat$mask),]
```

```{r}
### 2022
file_paths_tif <- list.files("enviro datasets/rasters for grid predictions/", pattern = "\\.tif$", full.names = TRUE)
#make mask the first item in the vector
file_paths_tif2022<-c(file_paths_tif[18], file_paths_tif[-c(18,19,36,4,12,14,2)])
# Identify indices of elements containing "2010"
indices_to_remove <- grep("2010", file_paths_tif2022)

# Exclude those elements from the vector
file_paths_tif2022 <- file_paths_tif2022[-indices_to_remove]

# # e.g. Find the column number for "Age"
# column_number <- which(names(df) == "Age")
variable.names2022 <- c("mask", names(train_data_b)[c(15,9,8,37,12,17,14,33,10,35,36,26,29,20,21,46,24,18,34,47,25,19,32)]) #here make sure the order is the same as above, if you're using different data
processed_layers2022 <- list() # Initialize an empty list to store processed layers

for(i in 1:length(file_paths_tif2022)) { 
  # Load each .tif file as a RasterLayer object
  r <- raster(file_paths_tif2022[i])
  cat(file_paths_tif2022[i], "\n")
  if (variable.names2022[i] %in% c("COMMUNITY", "geohab", "geomorph", "builtfeature")) {
    r_agg_raster<-r
  } else {
    

  # Convert RasterLayer to SpatRaster for more efficient processing
  r_spat <- terra::rast(r)
  
  # Aggregate the SpatRaster object
  r_agg <- aggregate(r_spat, fact=5, fun=mean, na.rm=TRUE)
  
  # Optionally: Convert back to RasterLayer if needed for compatibility with other raster package functions
  r_agg_raster <- raster(r_agg)
  }
  # Replace nodata value with NA - Note: Adjust according to your data's needs
  currentNodata <- NAvalue(r_agg_raster)
  cat("NoData Value: ", currentNodata, "\n") # all seem to be -Inf
  
  # Store the aggregated RasterLayer object in the list with the desired name
  processed_layers2022[[variable.names2022[i]]] <- r_agg_raster
  
  # Optionally, assign the RasterLayer object to the global environment with the desired name
  assign(variable.names2022[i], r_agg_raster, envir = .GlobalEnv)
}
rasters_values2022 <- lapply(variable.names2022, function(name) values(get(name)))
preddat2022 <- data.frame(rasters_values2022) 
names(preddat2022) <- variable.names2022
preddat2022<- preddat2022[!is.na(preddat2022$mask),]
# this step takes a really long time
# do I need to do something about NAs/ nodata values being different in different layers? nop



#categorical variables - obtain factor levels
COMMUNITY1<-terra::vect("enviro datasets/NESP predicted distribution of seagrass communities/Shapefiles/GBR_NESP-TWQ-5.4_JCU_Seagrass-communities_20201203.shp")
levels<-levels(as.factor(COMMUNITY1$COMMUNITY))
# Recover the original factor levels for the aggregated raster
preddat2022$COMMUNITY<-factor(preddat2022$COMMUNITY, levels=1:length(levels), labels=levels)
preddat$COMMUNITY<-factor(preddat$COMMUNITY, levels=1:length(levels), labels=levels)

builtfeature<-terra::vect("enviro datasets/Breakwaters, groynes, seawalls, marinas/QSC_Extracted_Data_20231018_085138072000-40848/data.gdb")
levels<-levels(as.factor(builtfeature$FEATURETYPE))
preddat2022$builtfeature<- factor(preddat2022$builtfeature, levels=1:length(levels), labels=levels)
preddat$builtfeature<- factor(preddat$builtfeature, levels=1:length(levels), labels=levels)

geohab<-terra::vect("enviro datasets/qld_geohab_av/czm/pristine_ests/release/qld/shape/geohab_qld_v2.shp")
levels<-levels(as.factor(geohab$GH_TYPE))
preddat2022$geohab<-factor(preddat2022$geohab, levels=1:length(levels), labels=levels)
preddat$geohab<-factor(preddat$geohab, levels=1:length(levels), labels=levels)

geomorph<-terra::vect("enviro datasets/Heap2008Geomorphology.shp")
levels<-levels(as.factor(geomorph$feature))
preddat2022$geomorph<-factor(preddat2022$geomorph, levels=1:length(levels), labels=levels)
preddat$geomorph<-factor(preddat$geomorph, levels=1:length(levels), labels=levels)

write.csv(preddat, "preddat.grid.background.csv")
write.csv(preddat2022, "preddat2022.grid.background.csv")
# preddat<- preddat[!is.na(mask),]

#?names(preddat2022)[-c(1,2)] <- variable.names2022[2:length(variable.names2022)]
#names(preddat)[-c(1,2)] <- variable.names2010[2:length(variable.names2010)]
```

```{r}
preddat<-read.csv("preddat.grid.background.csv")
preddat2022<-read.csv("preddat2022.grid.background.csv")
```
```{r}
raster_extent <- extent(mask)

# Extract XLLCORNER and YLLCORNER
xllcorner <- xmin(raster_extent)
yllcorner <- ymin(raster_extent)
```


```{r}
source("Elith 2008 tutorial/brt.functions.R")
unmodgrid2010<-gbm.predict.grids(BRT.backgroundunmod, # change this when you have all the rasters, this step takes a really long time
                  preddat, 
                  want.grids = T, 
                  sp.name ="backgroundunmod_preds",
                  pred.vec = rep(-9999,20428380), # adjust number of rows in your data
                  filepath = "brtpred/", 
                  num.col =4340, 
                  num.row = 4707, 
                  xll = xllcorner, # make sure exports have the right spatial offset
                  yll = yllcorner, 
                  cell.size = 500, 
                  no.data = -9999,
                  plot=T)
saveRDS(unmodgrid2010, "grid2010unmod2.Rds")

modgrid2010<-gbm.predict.grids(BRT.backgroundGL, 
                  preddat, 
                  want.grids = T, 
                  sp.name ="backgroundGL_preds",
                  pred.vec = rep(-9999,20428380), # adjust number of rows in your data
                  filepath = "brtpred/", 
                  num.col =4340, 
                  num.row = 4707, 
                  xll = xllcorner, 
                  yll = yllcorner, 
                  cell.size = 500, 
                  no.data = -9999,
                  plot=T)
saveRDS(modgrid2010, "grid2010mod2.Rds")
```
```{r}
unmodgrid2022<-gbm.predict.grids(BRT.backgroundunmod, # change this when you have all the rasters, this step takes a really long time
                  preddat2022, 
                  want.grids = T, 
                  sp.name ="backgroundunmod_preds2022",
                  pred.vec = rep(-9999,20428380), # adjust number of rows in your data
                  filepath = "brtpred/", 
                  num.col =4340, 
                  num.row = 4707, 
                  xll = xllcorner, 
                  yll = yllcorner, 
                  cell.size = 500, 
                  no.data = -9999,
                  plot=T)
saveRDS(unmodgrid2022, "grid2022unmod2.Rds")

modgrid2022<-gbm.predict.grids(BRT.backgroundGL, 
                  preddat2022, 
                  want.grids = T, 
                  sp.name ="backgroundGL_preds2022",
                  pred.vec = rep(-9999,20428380), # adjust number of rows in your data
                  filepath = "brtpred/", 
                  num.col =4340, 
                  num.row = 4707, 
                  xll = xllcorner, 
                  yll = yllcorner, 
                  cell.size = 500, 
                  no.data = -9999,
                  plot=T)
saveRDS(modgrid2022, "grid2022mod2.Rds")
```
### crw
```{r}
dist2coast<-dist2coast1_2
preddat$dist2coast<-preddat$dist2coast1_2
preddat2022$dist2coast<-preddat2022$dist2coast1_2
unmodgrid2010crw<-gbm.predict.grids(BRT.crwunmod, # change this when you have all the rasters, this step takes a really long time
                  preddat, 
                  want.grids = T, 
                  sp.name ="crwunmod_preds",
                  pred.vec = rep(-9999,20428380), # adjust number of rows in your data
                  filepath = "brtpred/", 
                  num.col =4340, 
                  num.row = 4707, 
                  xll = xllcorner, 
                  yll = yllcorner, 
                  cell.size = 500, 
                  no.data = -9999,
                  plot=T)
saveRDS(unmodgrid2010crw, "grid2010unmodcrw.Rds")

modgrid2010crw<-gbm.predict.grids(BRT.crwGL, 
                  preddat, 
                  want.grids = T, 
                  sp.name ="crwGL_preds",
                  pred.vec = rep(-9999,20428380), # adjust number of rows in your data
                  filepath = "brtpred/", 
                  num.col =4340, 
                  num.row = 4707, 
                  xll = xllcorner, 
                  yll = yllcorner, 
                  cell.size = 500, 
                  no.data = -9999,
                  plot=T)
saveRDS(modgrid2010crw, "grid2010modcrw.Rds")
```
```{r}
unmodgrid2022crw<-gbm.predict.grids(BRT.crwunmod, # change this when you have all the rasters, this step takes a really long time
                  preddat2022, 
                  want.grids = T, 
                  sp.name ="crwunmod_preds2022",
                  pred.vec = rep(-9999,20428380), # adjust number of rows in your data
                  filepath = "brtpred/", 
                  num.col =4340, 
                  num.row = 4707, 
                  xll = xllcorner, 
                  yll = yllcorner, 
                  cell.size = 500, 
                  no.data = -9999,
                  plot=T)
saveRDS(unmodgrid2022crw, "grid2022unmodcrw.Rds")

modgrid2022crw<-gbm.predict.grids(BRT.crwGL, 
                  preddat2022, 
                  want.grids = T, 
                  sp.name ="crwGL_preds2022",
                  pred.vec = rep(-9999,20428380), # adjust number of rows in your data
                  filepath = "brtpred/", 
                  num.col =4340, 
                  num.row = 4707, 
                  xll = xllcorner, 
                  yll = yllcorner, 
                  cell.size = 500, 
                  no.data = -9999,
                  plot=T)
saveRDS(modgrid2022crw, "grid2022modcrw.Rds")
```

```{r}

unmodgrid2022crw<-readRDS("grid2022unmodcrw.Rds")

unmodgrid2010crw<-readRDS("grid2010unmodcrw.Rds")

modgrid2022crw<-readRDS("grid2022modcrw.Rds")
modgrid2010crw<-readRDS("grid2010modcrw.Rds")

# how much area in 2010 vs 2022
sum(unmodgrid2022crw>0.5)*250000/1000000
sum(unmodgrid2010crw>0.5)*250000/1000000
sum(modgrid2022crw<0.5)*250000/1000000
sum(modgrid2010crw<0.5)*250000/1000000
```
```{r}
#% within 50km of coastline
16484/sum(unmodgrid2010crw>0.5)
19132/sum(unmodgrid2022crw>0.5)
```


# Difference evaluation
Area? 
```{r}
sum(backgroundGL_preds>0.5) # number of 500m pixels with more than 50% probability of presence
sum(backgroundGL_preds2022>0.5) # way less in 2022
sum(backgroundunmod_preds>0.5) 
sum(backgroundunmod_preds2022>0.5) # way less in 2022

sum(crwGL_preds2022<0.5) # number of 500m pixels with more than 50% probability of presence
sum(crwunmod_preds2022>0.5) # way less in 2022, 34608
sum(crwunmod_preds>0.5) 
sum(crwunmod_preds2022>0.5) # way less in 2022
```

```{r}
sum(preddat2022$deamangrove<5000, na.rm=T)
sum(preddat$deamangrove<5000, na.rm=T) # about the same

sum(preddat$ZooL_N<3, na.rm=T)
sum(preddat2022$ZooL_N<3, na.rm=T) # much more in 2022

sum(preddat$EFI<0.1, na.rm=T)
sum(preddat2022$EFI<0.1, na.rm=T) # a bit less in 2022, but not heaps
```

2010 vs 2022
```{r}
hist(preddat$ZooL_N)
hist(preddat2022$ZooL_N)

hist(preddat$salt)
hist(preddat2022$salt)

hist(preddat$Secchi)
hist(preddat2022$Secchi)
```



# % overlap with protected zones and ports
```{r}
zones<-st_read("enviro datasets/Great_Barrier_Reef_Marine_Park_Zoning/Great_Barrier_Reef_Marine_Park_Zoning.shp")
ports<-st_read("../enviro shapefiles/Port_ERMP/Ports_GBR.shp")

crwunmod2022<-raster("brtpred/crwunmod_preds2022.asc")
```
```{r}
# reproject
crs(crwunmod2022)<-"EPSG:3577"


zones <- st_transform(zones, crs(crwunmod2022))
ports<- st_transform(ports, crs(crwunmod2022))

```
```{r }
# counting pixels in polygons

library(exactextractr) #calculates the fraction of each pixel that lies within the polygon boundaries and uses this fraction in its calculations

# Perform the extraction
coverage_stats <- exact_extract(crwunmod2022, zones, fun = function(values, coverage_fraction) {
  sum(values>0.5&!is.na(values) * coverage_fraction)
}, progress = FALSE)

# `coverage_stats` will contain the count of pixels for each polygon. You might want to add this back to your polygon data
zones$pixel_count <- coverage_stats

# If you need to aggregate by an attribute, you can use dplyr as shown previously
zones <- st_as_sf(zones) # Ensure it's an sf object for dplyr operations
summary_df <- zones %>%
  group_by(TYPE) %>%
  summarise(total_pixels = sum(pixel_count, na.rm = TRUE))

pixels_greater_than_0_5 <- crwunmod2022 > 0.5

# Now, use getValues to retrieve the raster values of this logical raster
values <- getValues(pixels_greater_than_0_5)

# Since values are logical (TRUE/FALSE), summing them counts the TRUE values.
# TRUE values are coerced to 1, and FALSE values are coerced to 0.
# na.rm = TRUE is used here to remove NA values before the summation.
habitat <- sum(values, na.rm = TRUE)

summary_df$percentage_area<-round(summary_df$total_pixels/habitat,3)
print(summary_df)


### ports
coverage_stats <- exact_extract(crwunmod2022, ports, fun = function(values, coverage_fraction) {
  sum(values>0.5&!is.na(values) * coverage_fraction)
}, progress = FALSE)

# `coverage_stats` will contain the count of pixels for each polygon. You might want to add this back to your polygon data
ports$pixel_count <- coverage_stats
ports <- st_as_sf(ports) # Ensure it's an sf object for dplyr operations
summary_dfports <- ports %>%
  summarise(total_pixels = sum(pixel_count, na.rm = TRUE))


summary_dfports$percentage_area<-round(summary_dfports$total_pixels/habitat,3)
```
How much fell outside the marine park boundary (i.e. higher than mid water)
```{r}
GBRMP<-st_read("enviro datasets/Great Barrier Reef Marine Park Boundary.shp")
GBRMP <- st_transform(GBRMP, crs(crwunmod2022))
coverage_stats <- exact_extract(crwunmod2022, GBRMP, fun = function(values, coverage_fraction) {
  sum(values>0.5&!is.na(values) * coverage_fraction)
}, progress = FALSE)

# `coverage_stats` will contain the count of pixels for each polygon. You might want to add this back to your polygon data
GBRMP$pixel_count <- coverage_stats
GBRMP <- st_as_sf(GBRMP) # Ensure it's an sf object for dplyr operations
summary_dfGBRMP <- GBRMP %>%
  summarise(total_pixels = sum(pixel_count, na.rm = TRUE))


summary_dfGBRMP$percentage_area<-round(summary_dfGBRMP$total_pixels/habitat,3)
```

```{r}
# examining spatial overlay of layers
raster_df<-as.data.frame(crwunmod2022, xy=T)
ggplot() +
  geom_raster(data = raster_df, aes(x = x, y = y, fill = crwunmod_preds2022)) +
  geom_sf(data = zones, fill = NA, color = "blue") +
  geom_sf(data = GBRMP, fill = NA, color = "red") +
  geom_sf(data= ports, fill=NA, color="yellow")+
  scale_fill_viridis_c() +
  theme_minimal() +
  labs(title = "Raster and Polygon Overlay", fill = "Raster value") +
  theme(legend.position = "bottom")
```

